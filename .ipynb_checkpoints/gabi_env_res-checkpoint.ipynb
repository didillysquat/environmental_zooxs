{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gabi's env_res project\n",
    "This notebook will document the processing of the data for Gabi's MSc. project that was collected some time ago by Maren.\n",
    "The samples that were collected were supposed to be used to characterise the zooxs present before during and after a bleaching episode but the bleaching never happened and so the sample collection was somewhat abandoned. The samples will now be repurposed and used by Gabi for her MSc project. The object of which will be to compare the diversity of ITS2 sequences found in the corals to those found in the environmental samples.\n",
    "\n",
    "Originally, we had planned to look for ITS2 profiles found in the coral samples in the environmental samples. I spent a lot of time writing code to search for the ITS2 type profiles but in the end this approach was abandoned because we did have a big enough selection of corals to really capture the full diversity of DIV abundances. As such we weren't finding many of the profiles in the environmental samples. \n",
    "\n",
    "Instead of this approach we will now aim for a much more simplified approach which will purely look at the diversities of ITS2 sequences in the coral samples and compare this to the diverstiy of sequences found in the environmental samples.\n",
    "\n",
    "As mentioned above there was originally a time component to the original samples collected. You will see in the info file that there are different collection dates. We will not be using any of these time replicate samples and only using the samples that werwe colleceted at the same time as the first batch of corals. You will also see that there were an additional set of corals that were collected at a second date and did not have a plot number associated with them. These will also be discarded.\n",
    "\n",
    "There was also a spatial consideration to the original data collection with there being 5 plots and the aim was to see whether environmental resevoirs were more specific to coral samples between and among the plots. However, due to the minimal number of environmental samples we are not able to take into account this aspect of the study so the plot factor will be ignored. \n",
    "\n",
    "You will see that that leaves us with a total of about 50 corals (although several sponge samples need to be removed) and associated mucus samples, 5 water samples, 5 close sediment samples (sampled at the base of the coral shelf), 5 far sediment samples (sampled 2m from the base of the coral shelf), and 10 turf algae samples.\n",
    "\n",
    "__Update 05/08/18__ I have now included the data from the other time points. This is described in the Multi-date figure creation section. This is mainly due to the fact that they are collected very close together and the samples look somewhat similar. I think this will be easily defensible to reviewers.\n",
    "\n",
    "__Update 08/08/19__ There are some real hurdles to overcome with this data set in terms of comparing diversity. We are working with ITS2 sequence diversity as an unspoken proxy for Symbiodinium taxa. This causes all sorts of complicaitons especially considering that more basal sequences are likely to be shared and the more derived less so. It is also important to note that the more basal sequeces will generally be the more abundant sequences. We also need to take into account the fact that the host samples are going to have very selective pressures working on them than the environmental samples. Additionally we have a big difference in the absolute number of returned sequences between the coral, mucus and other samples. Finally we will also need to take into account that the sequence by sequence data we are working with has been through a MED decomposition. I have created a set of 5 figures that will hopefully do a fairly good job of evidencing for a discussion. I have had various evolutions of the figures along the way, including single date, multi date, splitting or not splitting the sediment samples. I have kept the code for each of these as I have gone along. I will not spend a lot of time annotating this notebook with markdown as I have done a fairly good job within the code.\n",
    "\n",
    "#### Sections\n",
    "* [Clean up data input](#Clean-up-data-input)\n",
    "* [Diversity bars sinlge date](#Diversity-bars-sinlge-date)\n",
    "* [Diversity bars all dates separate](#Diversity-bars-all-dates-separate)\n",
    "* [Diversity bars all dates grouped](#Diversity-bars-all-dates-grouped)\n",
    "* [Diversity bars all dates grouped and sed grouped](#Diversity-bars-all-dates-grouped-and-sed-grouped)\n",
    "* [Diversity stats one date](#Diversity-stats-one-date)\n",
    "* [Diversity stats all dates grouped](#Diversity-stats-all-dates-grouped)\n",
    "* [Diversity stats all dates grouped and sed grouped](#Diversity-stats-all-dates-grouped-and-sed-grouped)\n",
    "* [Venn one date no abund labels](#Venn-one-date-no-abund-labels)\n",
    "* [Venn all dates with abund labels](#Venn-all-dates-with-abund-labels)\n",
    "* [Rarefaction curves](#Rarefaction-curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up data input\n",
    "\n",
    "One of the first things to deal with is the info data that contains the metadata for the samples. This was in the form of an excel file that has some formating issues when exported in .csv (as always). We will clean that up below. * We can't type the ^M special character here so I will just say that we ran ```$sed 's/[,]+^M$//g' info_290819.csv > info_290819.csv```\n",
    "\n",
    "We have the SP outputs in this directory, the info.csv that needs cleaning up and the code for generating the figs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head info_290819.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head info_290819_fix.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed '/^\\s*$/d' info_290819_fix.csv > info_290819_fix_fix.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head info_290819_fix_fix.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail info_290819_fix_fix.csv | cat -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sed -r 's/^,+$//g' info_290819_fix_fix.csv > info_290819_fix_fix_fix.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tail info_290819_fix_fix_fix.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv info_290819_fix_fix_fix.csv info_300718.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm *2908*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "info_300718.csv is now our meta datafile that we will work with in the python processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head info_300718.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversity bars sinlge date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read in the info file and the SP output file__\n",
    "\n",
    "Then clean these up by dropping the samples that are not of the correct data. Then relate the SP output and info files so that they have the same names for the samples. Also pickle out the data_frames and have a check to see if they already exist.\n",
    "\n",
    "The warning spat out by pandas can be ignored in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    sp_output_df = pickle.load(open('sp_output_df.pickle', 'rb'))\n",
    "    QC_info_df= pickle.load(open('QC_info_df.pickle', 'rb'))\n",
    "    info_df = pickle.load(open('info_df.pickle', 'rb'))\n",
    "except:\n",
    "    # read in the SymPortal output\n",
    "    sp_output_df = pd.read_csv('131_142.DIVs.absolute.txt', sep='\\t', lineterminator='\\n')\n",
    "\n",
    "    # The SP output contains the QC info columns between the DIVs and the no_name ITS2 columns.\n",
    "    # lets put the QC info columns into a seperate df.\n",
    "    QC_info_df = sp_output_df[['Samples','raw_contigs', 'post_qc_absolute_seqs', 'post_qc_unique_seqs',\n",
    "                            'post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs',\n",
    "                            'post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs',\n",
    "                               'size_screening_violation_absolute', 'size_screening_violation_unique',\n",
    "                               'post_med_absolute', 'post_med_unique']]\n",
    "\n",
    "    # now lets drop the QC columns from the SP output df and also drop the clade summation columns\n",
    "    # we will be left with just clumns for each one of the sequences found in the samples\n",
    "    sp_output_df.drop(columns=['noName Clade A', 'noName Clade B', 'noName Clade C', 'noName Clade D',\n",
    "                            'noName Clade E', 'noName Clade F', 'noName Clade G', 'noName Clade H',\n",
    "                            'noName Clade I', 'raw_contigs', 'post_qc_absolute_seqs', 'post_qc_unique_seqs',\n",
    "                            'post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs',\n",
    "                            'post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs',\n",
    "                               'size_screening_violation_absolute', 'size_screening_violation_unique',\n",
    "                               'post_med_absolute', 'post_med_unique'\n",
    "                               ]\n",
    "                      , inplace=True)\n",
    "\n",
    "    # read in the info file\n",
    "    info_df = pd.read_csv('info_300718.csv')\n",
    "\n",
    "    # drop the rows that aren't from the 22.08.2016 data\n",
    "    info_df = info_df[info_df['date collected'] == '22.08.2016']\n",
    "\n",
    "    # Now we need to link the SP output to the sample names in the excel. Annoyingly they are formatted\n",
    "    # slightly differently so we can't make a direct comparison.\n",
    "    # easiest way to link them is to see if the first part of the SP name is the same as the first part\n",
    "    # of the 'sequence file' in the meta info\n",
    "    # when doing this we can also drop the SP info for those samples that won't be used i.e. those that\n",
    "    # aren't now in the info_df\n",
    "\n",
    "    # firstly rename the colums so that they are 'sample_name' in all of the dfs\n",
    "    QC_info_df.rename(index=str, columns={'Samples': 'sample_name'}, inplace=True)\n",
    "    sp_output_df.rename(index=str, columns={'Samples': 'sample_name'}, inplace=True)\n",
    "    info_df.rename(index=str, columns={'Sample Name': 'sample_name'}, inplace=True)\n",
    "\n",
    "    indices_to_drop = []\n",
    "    for sp_index in sp_output_df.index.values.tolist():\n",
    "        # keep track of whether the sp_index was found in the info table\n",
    "        # if it wasn't then it should be dropped\n",
    "        found = False\n",
    "        for info_index in info_df.index.values.tolist():\n",
    "            if sp_output_df.loc[sp_index, 'sample_name'].split('_')[0] == info_df.loc[info_index, 'Sequence file'].split('_')[0]:\n",
    "                found = True\n",
    "                # then these are a related set of rows and we should make the sample_names the same\n",
    "                sp_output_df.loc[sp_index, 'sample_name'] = info_df.loc[info_index, 'sample_name']\n",
    "                QC_info_df.loc[sp_index, 'sample_name'] = info_df.loc[info_index, 'sample_name']\n",
    "\n",
    "\n",
    "        if not found:\n",
    "            indices_to_drop.append(sp_index)\n",
    "\n",
    "    # drop the rows from the SP output tables that aren't going to be used\n",
    "    sp_output_df.drop(inplace=True, index=indices_to_drop)\n",
    "    QC_info_df.drop(inplace=True, index=indices_to_drop)\n",
    "\n",
    "    # let's sort out the 'environ type' column in the info_df\n",
    "    # currently it is a bit of a mess\n",
    "    for index in info_df.index.values.tolist():\n",
    "        if 'coral' in info_df.loc[index, 'Sample no.']:\n",
    "            info_df.loc[index, 'environ type'] = 'coral'\n",
    "        elif 'seawater' in info_df.loc[index, 'Sample no.']:\n",
    "            info_df.loc[index, 'environ type'] = 'sea_water'\n",
    "        elif 'mucus' in info_df.loc[index, 'Sample no.']:\n",
    "            info_df.loc[index, 'environ type'] = 'mucus'\n",
    "        elif 'SA' in info_df.loc[index, 'Sample no.']:\n",
    "            info_df.loc[index, 'environ type'] = 'sed_close'\n",
    "        elif 'SB' in info_df.loc[index, 'Sample no.']:\n",
    "            info_df.loc[index, 'environ type'] = 'sed_far'\n",
    "        elif 'TA' in info_df.loc[index, 'Sample no.']:\n",
    "            info_df.loc[index, 'environ type'] = 'turf'\n",
    "\n",
    "    # now clean up the df indices\n",
    "    info_df.index = range(len(info_df))\n",
    "    sp_output_df.index = range(len(sp_output_df))\n",
    "    QC_info_df.index = range(len(QC_info_df))\n",
    "\n",
    "    # make the sample_name column the index for each of the datasets\n",
    "    info_df.set_index('sample_name', inplace=True)\n",
    "    sp_output_df.set_index('sample_name', inplace=True)\n",
    "    QC_info_df.set_index('sample_name', inplace=True)\n",
    "\n",
    "    # pickle the out put and put a check in place to see if we need to do the above\n",
    "    pickle.dump(sp_output_df, open('sp_output_df.pickle', 'wb'))\n",
    "    pickle.dump(QC_info_df, open('QC_info_df.pickle', 'wb'))\n",
    "    pickle.dump(info_df, open('info_df.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sp_output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(QC_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq_rel_abund_calculator_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create dictionary that will contain seq_name as key, value = tuple of (sample name with highest abundnce of the sequence, the rel abundance of the sequence)__\n",
    "\n",
    "See comments in code for further explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we want to plot the ITS2 sequence diversity in each of the samples as bar charts\n",
    "# We are going to have a huge diversity of sequences to deal with so I think something along the lines\n",
    "# of plotting the top n most abundant sequences. The term 'most abundant' should be considered carefully here\n",
    "# I think it will be best if we work on a sample by sample basis. i.e. we pick the n sequences that have the\n",
    "# highest representation in any one sample. So for example what we are not doing is seeing how many times\n",
    "# C3 was sequenced across all of the samples, and finding that it is a lot and therefore plotting it.\n",
    "# we are looking in each of the samples and seeing the highest proportion it is found at in any one sample.\n",
    "# This way we should have the best chance of having a coloured representation for each sample's most abundant\n",
    "# sequence.\n",
    "\n",
    "# to start lets go sample by sample and see what the highest prop for each seq is.\n",
    "\n",
    "# dict to hold info on which sample and what the proportion is for each sequence\n",
    "# key = sequence name, value = tup ( sample name, relative abundance)\n",
    "try:\n",
    "    seq_rel_abund_calculator_dict = pickle.load(open('seq_rel_abund_calculator_dict.pickle', 'rb'))\n",
    "except:\n",
    "    seq_rel_abund_calculator_dict = {}\n",
    "    for sample_index in sp_output_df.index.values.tolist():\n",
    "        sys.stdout.write('\\nGeting rel seq abundances from {}\\n'.format(sample_index))\n",
    "        # temp_prop_array = sp_output_df.loc[sample_index].div(sp_output_df.loc[sample_index].sum(axis='index'))\n",
    "        temp_prop_array = sp_output_df.loc[sample_index].div(sp_output_df.loc[sample_index].sum())\n",
    "        for seq_name in temp_prop_array.keys():\n",
    "            sys.stdout.write('\\rseq: {}'.format(seq_name))\n",
    "            val = temp_prop_array[seq_name]\n",
    "            if val != 0:  # if the sequences was found in the sample\n",
    "                # If the sequence is already in the dict\n",
    "                if seq_name in seq_rel_abund_calculator_dict.keys():\n",
    "                    # check to seee if the rel abundance is larger than the one already logged\n",
    "                    if val > seq_rel_abund_calculator_dict[seq_name][1]:\n",
    "                        seq_rel_abund_calculator_dict[seq_name] = (sample_index, val)\n",
    "                # if we haven't logged for this sequence yet, then add this as the first log\n",
    "                else:\n",
    "                    seq_rel_abund_calculator_dict[seq_name] = (sample_index, val)\n",
    "    pickle.dump(seq_rel_abund_calculator_dict, open('seq_rel_abund_calculator_dict.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create a sorted list of sequences where the first sequence has the highest rel abundance in any one given sample__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we have a dict that contains the largest rel_abundances per sample for each of the seqs\n",
    "# now we can sort this to look at the top ? n sequences to start with (I'm not sure how the colouring will\n",
    "# look like so lets just start with 30 and see where we get to)\n",
    "sorted_list = sorted(seq_rel_abund_calculator_dict.items(), key = lambda x: x[1][1], reverse=True)\n",
    "most_abund_seq_names = [tup[0] for tup in sorted_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__From this list create a colour dictionary which will hold the info for which sequences should be which colour and will keep the colours consistent between each of the subplots__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colour_list():\n",
    "    colour_list = [\"#FFFF00\", \"#1CE6FF\", \"#FF34FF\", \"#FF4A46\", \"#008941\", \"#006FA6\", \"#A30059\", \"#FFDBE5\",\n",
    "                  \"#7A4900\", \"#0000A6\", \"#63FFAC\", \"#B79762\", \"#004D43\", \"#8FB0FF\", \"#997D87\", \"#5A0007\", \"#809693\",\n",
    "                  \"#FEFFE6\", \"#1B4400\", \"#4FC601\", \"#3B5DFF\", \"#4A3B53\", \"#FF2F80\", \"#61615A\", \"#BA0900\", \"#6B7900\",\n",
    "                  \"#00C2A0\", \"#FFAA92\", \"#FF90C9\", \"#B903AA\", \"#D16100\", \"#DDEFFF\", \"#000035\", \"#7B4F4B\", \"#A1C299\",\n",
    "                  \"#300018\", \"#0AA6D8\", \"#013349\", \"#00846F\", \"#372101\", \"#FFB500\", \"#C2FFED\", \"#A079BF\", \"#CC0744\",\n",
    "                  \"#C0B9B2\", \"#C2FF99\", \"#001E09\", \"#00489C\", \"#6F0062\", \"#0CBD66\", \"#EEC3FF\", \"#456D75\", \"#B77B68\",\n",
    "                  \"#7A87A1\", \"#788D66\", \"#885578\", \"#FAD09F\", \"#FF8A9A\", \"#D157A0\", \"#BEC459\", \"#456648\", \"#0086ED\",\n",
    "                  \"#886F4C\", \"#34362D\", \"#B4A8BD\", \"#00A6AA\", \"#452C2C\", \"#636375\", \"#A3C8C9\", \"#FF913F\", \"#938A81\",\n",
    "                  \"#575329\", \"#00FECF\", \"#B05B6F\", \"#8CD0FF\", \"#3B9700\", \"#04F757\", \"#C8A1A1\", \"#1E6E00\", \"#7900D7\",\n",
    "                  \"#A77500\", \"#6367A9\", \"#A05837\", \"#6B002C\", \"#772600\", \"#D790FF\", \"#9B9700\", \"#549E79\", \"#FFF69F\",\n",
    "                  \"#201625\", \"#72418F\", \"#BC23FF\", \"#99ADC0\", \"#3A2465\", \"#922329\", \"#5B4534\", \"#FDE8DC\", \"#404E55\",\n",
    "                  \"#0089A3\", \"#CB7E98\", \"#A4E804\", \"#324E72\", \"#6A3A4C\", \"#83AB58\", \"#001C1E\", \"#D1F7CE\", \"#004B28\",\n",
    "                  \"#C8D0F6\", \"#A3A489\", \"#806C66\", \"#222800\", \"#BF5650\", \"#E83000\", \"#66796D\", \"#DA007C\", \"#FF1A59\",\n",
    "                  \"#8ADBB4\", \"#1E0200\", \"#5B4E51\", \"#C895C5\", \"#320033\", \"#FF6832\", \"#66E1D3\", \"#CFCDAC\", \"#D0AC94\",\n",
    "                  \"#7ED379\", \"#012C58\", \"#7A7BFF\", \"#D68E01\", \"#353339\", \"#78AFA1\", \"#FEB2C6\", \"#75797C\", \"#837393\",\n",
    "                  \"#943A4D\", \"#B5F4FF\", \"#D2DCD5\", \"#9556BD\", \"#6A714A\", \"#001325\", \"#02525F\", \"#0AA3F7\", \"#E98176\",\n",
    "                  \"#DBD5DD\", \"#5EBCD1\", \"#3D4F44\", \"#7E6405\", \"#02684E\", \"#962B75\", \"#8D8546\", \"#9695C5\", \"#E773CE\",\n",
    "                  \"#D86A78\", \"#3E89BE\", \"#CA834E\", \"#518A87\", \"#5B113C\", \"#55813B\", \"#E704C4\", \"#00005F\", \"#A97399\",\n",
    "                  \"#4B8160\", \"#59738A\", \"#FF5DA7\", \"#F7C9BF\", \"#643127\", \"#513A01\", \"#6B94AA\", \"#51A058\", \"#A45B02\",\n",
    "                  \"#1D1702\", \"#E20027\", \"#E7AB63\", \"#4C6001\", \"#9C6966\", \"#64547B\", \"#97979E\", \"#006A66\", \"#391406\",\n",
    "                  \"#F4D749\", \"#0045D2\", \"#006C31\", \"#DDB6D0\", \"#7C6571\", \"#9FB2A4\", \"#00D891\", \"#15A08A\", \"#BC65E9\",\n",
    "                  \"#FFFFFE\", \"#C6DC99\", \"#203B3C\", \"#671190\", \"#6B3A64\", \"#F5E1FF\", \"#FFA0F2\", \"#CCAA35\", \"#374527\",\n",
    "                  \"#8BB400\", \"#797868\", \"#C6005A\", \"#3B000A\", \"#C86240\", \"#29607C\", \"#402334\", \"#7D5A44\", \"#CCB87C\",\n",
    "                  \"#B88183\", \"#AA5199\", \"#B5D6C3\", \"#A38469\", \"#9F94F0\", \"#A74571\", \"#B894A6\", \"#71BB8C\", \"#00B433\",\n",
    "                  \"#789EC9\", \"#6D80BA\", \"#953F00\", \"#5EFF03\", \"#E4FFFC\", \"#1BE177\", \"#BCB1E5\", \"#76912F\", \"#003109\",\n",
    "                  \"#0060CD\", \"#D20096\", \"#895563\", \"#29201D\", \"#5B3213\", \"#A76F42\", \"#89412E\", \"#1A3A2A\", \"#494B5A\",\n",
    "                  \"#A88C85\", \"#F4ABAA\", \"#A3F3AB\", \"#00C6C8\", \"#EA8B66\", \"#958A9F\", \"#BDC9D2\", \"#9FA064\", \"#BE4700\",\n",
    "                  \"#658188\", \"#83A485\", \"#453C23\", \"#47675D\", \"#3A3F00\", \"#061203\", \"#DFFB71\", \"#868E7E\", \"#98D058\",\n",
    "                  \"#6C8F7D\", \"#D7BFC2\", \"#3C3E6E\", \"#D83D66\", \"#2F5D9B\", \"#6C5E46\", \"#D25B88\", \"#5B656C\", \"#00B57F\",\n",
    "                  \"#545C46\", \"#866097\", \"#365D25\", \"#252F99\", \"#00CCFF\", \"#674E60\", \"#FC009C\", \"#92896B\"]\n",
    "    return colour_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the above sorted list we can then plot these sequences with colour and all others with grey scale\n",
    "# lets make a coulour dictionary for the most common types\n",
    "colour_list = get_colour_list()\n",
    "colour_dict = {}\n",
    "num_coloured_seqs = 30\n",
    "\n",
    "# we will also need a grey palette for those sequences that are not the ones being annotated\n",
    "grey_palette = ['#D0CFD4', '#89888D', '#4A4A4C', '#8A8C82', '#D4D5D0', '#53544F']\n",
    "\n",
    "# make the dict\n",
    "for i in range(len(most_abund_seq_names)):\n",
    "    if i < num_coloured_seqs:\n",
    "        colour_dict[most_abund_seq_names[i]] = colour_list[i]\n",
    "    else:\n",
    "        colour_dict[most_abund_seq_names[i]] = grey_palette[i % 6]\n",
    "# add the 'low' key and assign it to grey for later on\n",
    "# the low category will be created later on to hold the grouped abundances of sequences in samples\n",
    "# below a certain rel abund cutoff\n",
    "colour_dict['low'] = '#D0CFD4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Set up the structures for collecting the rectangle objects we will use for making the legend later on__\n",
    "\n",
    "See comments in the code for further details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plotting we will also need to collect the 'rectangle' objects from the plotting process to use to make\n",
    "# the lengend which we will display on the plot right at the end\n",
    "# this is going to be a little tricky to collect as not ever sample type group that we are plotting\n",
    "# is going to have all of the top n sequences. So, we will have to pick up rectangles when they come up in\n",
    "# the plotting.\n",
    "# to keep track of which sequences we need we have:\n",
    "top_n_seqs_to_get = most_abund_seq_names[:num_coloured_seqs]\n",
    "# to keep track of which sequences we have already collected we have:\n",
    "top_n_seqs_done = []\n",
    "#finally to collect the rectangles and store them in order despite collecting them out of order we have:\n",
    "legend_rectangle_holder = [[] for i in range(num_coloured_seqs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Setup the figure as a set of sub plots, one for each of the sample types__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Do the plotting sample type by sample type__\n",
    "\n",
    "This will also involve collecting all of the information for plotting. We will do a little bit of processing here whereby we will group all sequences below a given rel abun cut off into a group called 'low'. If we don't do this then we simply end up with too many sequences to plot and work with. I think I am currently running with a rel_abund cutoff of 0.005 which is already pretty low. This leaves us with ~200 sequences to plot for the coral samples, a lot less for the other sample types e.g. turf.\n",
    "\n",
    "Normally I would make a 2D list that I would collect the plotting information in but in this case we will make a dataframe taht will only hold the information for the sample type in question.\n",
    "\n",
    "Because this code in in a for loop it will be a lot of code in one go but hopefully the comments are good enough to see what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating plotting info for coral samples\n",
      "196/197\n",
      "Generating plotting info for mucus samples\n",
      "190/191\n",
      "Generating plotting info for sea_water samples\n",
      "58/595\n",
      "Generating plotting info for sed_close samples\n",
      "87/889\n",
      "Generating plotting info for sed_far samples\n",
      "80/810\n",
      "Generating plotting info for turf samples\n",
      "72/730"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set up the plotting environment\n",
    "# one plot for coral, mucus, seawater, sediment, turf_algae\n",
    "f, axarr = plt.subplots(6, 1)\n",
    "# counter to reference which set of axes we are plotting on\n",
    "axarr_index = 0\n",
    "\n",
    "# now we need to get the actual plotting information for each of the samples.\n",
    "# we can do this sample type by sample type\n",
    "# we will create a local dataframe that will be a sub set of the main sp output dataframe but will\n",
    "# only contain the samples of the given sample type. It will also eventually only contain the sequence\n",
    "# information for the sample type in question. Normally I would make a 2D list to hold the plotting\n",
    "# information but in this case having all of the informatin in a dataframe is working really well and\n",
    "# this is how I will do it in future.\n",
    "\n",
    "# go environment type by environment type\n",
    "for env_type in ['coral', 'mucus', 'sea_water', 'sed_close', 'sed_far', 'turf']:\n",
    "    sys.stdout.write('\\nGenerating plotting info for {} samples\\n'.format(env_type))\n",
    "    # currently we have something like 4000 sequences to plot which is too may\n",
    "    # I think it will be much easier if we group the sequences that are found below a certain\n",
    "    # threshold. I think the best way to do this is to create a slice of the main df that\n",
    "    # contain the information for the samples of the env_type only\n",
    "\n",
    "    # get subset of the main dfs that contain only the coral samples\n",
    "    env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "    env_sp_output_df = sp_output_df.loc[env_info_df.index.values.tolist()]\n",
    "\n",
    "    # append a 'low' columns to the env_sp_ouput_df and populate with 0s\n",
    "    env_sp_output_df['low'] = 0\n",
    "    # now make a proportions version of the df, rather than absolute counts\n",
    "    env_sp_output_df_prop = env_sp_output_df[:].div(env_sp_output_df[:].sum(axis=1), axis=0)\n",
    "\n",
    "    # now as we work our way through we can sum up the low sequences into this column\n",
    "    # we can then check for 0 columns and drop these.\n",
    "\n",
    "    # get a list of the sequences found in the collection of samples of the given type and order\n",
    "    # them according to summed rel_abundance acorss all samples. This should be the order in which\n",
    "    # the samples are plotted\n",
    "    # at the same time we can get the info we need for plotting\n",
    "\n",
    "    summed_seq_rel_abund_across_smpls_dict = {seq: 0 for seq in list(sp_output_df)}\n",
    "\n",
    "    # we are also going to need to put the samples in an order that makes sense.\n",
    "    # Ideally I would like to take the time to do a proper travelling salesman analysis\n",
    "    # and I was thinking of putting in an ant colony system to do this but...\n",
    "    # I'm not sure we've got time for that. So let's do something a little more simple which\n",
    "    # should still be effective and look groovy\n",
    "    # Lets sort according to the majority sequence. I.e. lets put the samples into groups that\n",
    "    # are defined by what their majorty sequence is, then lets plot in order of those groups.\n",
    "    # within the groups we will plot in the order of the most abundant rel abund first.\n",
    "    # to get this information we will have a dict to collect it. The key of the dict will\n",
    "    # be the majority sequence with the value being a list that contains tuples. One tuple\n",
    "    # for each sample in the list which will contain the sample_name and the re abund of the maj seq\n",
    "\n",
    "    # Dict for holding the sample_sorting info\n",
    "    sample_sorting_info_dict = defaultdict(list)\n",
    "\n",
    "    for sp_index in env_sp_output_df_prop.index.values.tolist():\n",
    "        sys.stdout.write('\\r{}'.format(sp_index))\n",
    "        # we need to get the name of the most abundant sequence and its rel abund for each sample\n",
    "        # for the sample_sorting_info_dict\n",
    "        most_abund_seq_name = env_sp_output_df_prop.loc[sp_index].idxmax(axis='index')\n",
    "        rel_abund_of_most_abund_seq = env_sp_output_df_prop.loc[sp_index, most_abund_seq_name]\n",
    "        sample_sorting_info_dict[most_abund_seq_name].append((sp_index, rel_abund_of_most_abund_seq))\n",
    "        # Put its seq rel abundances ino the summed_seq_rel_abund_across... dict\n",
    "\n",
    "        for non_zero_seq in env_sp_output_df_prop.loc[sp_index][env_sp_output_df_prop.loc[sp_index] > 0].index:  # not including the final 'low' columns (this will be zero)\n",
    "            val = env_sp_output_df_prop.loc[sp_index, non_zero_seq]\n",
    "            # be sure to count the value of this cell and using it in judging which are the most\n",
    "            # abundant sequences before we check whether to relegate it to the 'low' column\n",
    "            summed_seq_rel_abund_across_smpls_dict[non_zero_seq] += val\n",
    "            if val < 0.005:\n",
    "                env_sp_output_df_prop.loc[sp_index, 'low'] += val\n",
    "                env_sp_output_df_prop.loc[sp_index, non_zero_seq] = 0\n",
    "\n",
    "\n",
    "    # here we can get a sorted sample list using the sample_sorting_info_dict\n",
    "    sorted_sample_list = []\n",
    "    # we want to work through the sample_sorting_info_dict by the longest lists first\n",
    "    sorted_keys_for_sample_sort = \\\n",
    "        [tup[0] for tup in sorted(sample_sorting_info_dict.items(), key=lambda x: len(x[1]), reverse=True)]\n",
    "    # for each of teh maj seq groups\n",
    "    for sorted_key in sorted_keys_for_sample_sort:\n",
    "        # now within each of these lists we want to order according to the rel_abundance of the sequences\n",
    "        sorted_list_of_samples_of_group = [tup[0] for tup in sorted(sample_sorting_info_dict[sorted_key], key=lambda x: x[1], reverse=True)]\n",
    "        sorted_sample_list.extend(sorted_list_of_samples_of_group)\n",
    "\n",
    "    # now we should re-order the df so that it is in the sample order of sorted_sample_list\n",
    "    env_sp_output_df_prop = env_sp_output_df_prop.reindex(sorted_sample_list)\n",
    "\n",
    "    # here we have a dict that contains the abundances of the seqs for the coral samples\n",
    "    # we will plot the coral samples' seqs in the order of the sequences in this dict\n",
    "    sorted_list_of_env_specific_seqs_tup \\\n",
    "        = sorted(summed_seq_rel_abund_across_smpls_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    sorted_list_of_env_specific_seqs = [tup[0] for tup in sorted_list_of_env_specific_seqs_tup]\n",
    "\n",
    "    # Now we check for zero only columns and drop them\n",
    "    # we also need to remove these columns from the sorted\n",
    "    non_zero_cols = list(env_sp_output_df_prop.loc[:, (env_sp_output_df_prop != 0).any(axis=0)])\n",
    "    sorted_list_of_env_specific_seqs = [seq for seq in sorted_list_of_env_specific_seqs if seq in non_zero_cols]\n",
    "\n",
    "    # add the 'low' column\n",
    "    sorted_list_of_env_specific_seqs.append('low')\n",
    "\n",
    "    # now drop the cols\n",
    "    env_sp_output_df_prop = env_sp_output_df_prop[non_zero_cols]\n",
    "\n",
    "    # we also have the plotting list which contains the info we will be plotting.\n",
    "\n",
    "    # the plotting_list is currently organised in a different order to that of the sorted_list_of_env...\n",
    "    # we need to change this order\n",
    "    env_sp_output_df_prop = env_sp_output_df_prop[sorted_list_of_env_specific_seqs]\n",
    "\n",
    "    # we now need to transpose this\n",
    "    env_sp_output_df_prop = env_sp_output_df_prop.transpose()\n",
    "\n",
    "    # here we finally have the plotting lists in the order of the sorted_list_of_env\n",
    "    bottom = [0 for smp in list(env_sp_output_df_prop)]\n",
    "    bar_list = []\n",
    "    # for each sample\n",
    "    plotting_indices = range(len(list(env_sp_output_df_prop)))\n",
    "\n",
    "    # for each sequence\n",
    "    list_of_seqs = env_sp_output_df_prop.index.values.tolist()\n",
    "    for i in range(len(list_of_seqs)):\n",
    "\n",
    "        sys.stdout.write('\\r{}/{}'.format(i, len(list(env_sp_output_df_prop.iloc[:, 0]))))\n",
    "        bar_list.append(axarr[axarr_index].bar(plotting_indices, list(env_sp_output_df_prop.iloc[i]), 1, bottom,\n",
    "                                               color=colour_dict[sorted_list_of_env_specific_seqs[i]]))\n",
    "        bottom = [L + M for L, M in zip(bottom, list(env_sp_output_df_prop.iloc[i]))]\n",
    "\n",
    "        # check to see if the seq we are plotting is still in the top_n_seqs list\n",
    "        # if it is in this list then we still need to grab a rectangle for plotting\n",
    "        # the legend. Once we have grabbed the rectangle then we should remove the seq from the top_n_seqs list\n",
    "        seq_name = list_of_seqs[i]\n",
    "        top_n_seqs_done = []\n",
    "        if seq_name in top_n_seqs_to_get and seq_name not in top_n_seqs_done:\n",
    "            # then this is a seq that we still need to get a rectangle for the legend\n",
    "            legend_rectangle_holder[top_n_seqs_to_get.index(seq_name)].append(bar_list[-1][0])\n",
    "            top_n_seqs_done.append(seq_name)\n",
    "\n",
    "    # https://stackoverflow.com/questions/12998430/remove-xticks-in-a-matplotlib-plot\n",
    "    axarr[axarr_index].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    # https://stackoverflow.com/questions/15858192/how-to-set-xlim-and-ylim-for-a-subplot-in-matplotlib\n",
    "    axarr[axarr_index].set_xlim((-0.5, (len(list(env_sp_output_df_prop)) - 0.5)))\n",
    "    # https://stackoverflow.com/questions/925024/how-can-i-remove-the-top-and-right-axis-in-matplotlib\n",
    "    axarr[axarr_index].spines['right'].set_visible(False)\n",
    "    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "\n",
    "    # To make the legend for this mo fo is going to be a little tricky. The legend argument basically takes\n",
    "    # two lists. The first list should contain a rectangle object for each of the sequences (this\n",
    "    # will be the coloured box). We get this object from the bar objects that we are creating.\n",
    "    # The second list is a list of the labels. This should be easier. We can just use the\n",
    "    # most_abund_seq_names[:30] for these.\n",
    "    # to grab the rectangles, I think its best to pick them up during the plotting and hold them in a list\n",
    "    # outside of the subplot loop. We will need a holder for the objects to populate.\n",
    "\n",
    "    #https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_xticks.html#matplotlib.axes.Axes.set_xticks\n",
    "    axarr[axarr_index].set_yticks([1], minor=False)\n",
    "    #https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html#matplotlib.axes.Axes.set_xticklabels\n",
    "    axarr[axarr_index].set_yticklabels(['1'])\n",
    "    axarr[axarr_index].set_xlabel(env_type)\n",
    "\n",
    "\n",
    "\n",
    "    axarr_index += 1\n",
    "\n",
    "#https://stackoverflow.com/questions/16150819/common-xlabel-ylabel-for-matplotlib-subplots/26892326\n",
    "f.text(0, 0.5, 'ITS2 sequence relative abundance', va='center', rotation='vertical')\n",
    "ordered_rectange_list = [sub_list[0] for sub_list in legend_rectangle_holder]\n",
    "# Uncomment when running locally to make the actual figure\n",
    "# f.legend(ordered_rectange_list, top_n_seqs_to_get, loc='lower center')\n",
    "\n",
    "plt.tight_layout()\n",
    "f.savefig('diversity_bars.svg')\n",
    "#Uncomment for running in local environment. Causes a crash in Jupyter\n",
    "#f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversity bars all dates separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diverstiy_figs_all_dates():\n",
    "    ''' This is a development from the above code. Here in this code we will plot all of the date replicates\n",
    "    for the env_samples as these could in theory aid a little bit in the diversity comparison.\n",
    "    I will put these on the same axis as the samples of the same type with a gap or so inbetween.\n",
    "    This will be the code to create the diversity figure for Gabi's paper.\n",
    "        We have used symportal to do the QC work on the samples.\n",
    "        As such we have tab delimited outputs that we can read into pandas dataframes\n",
    "        We also have the info list from Gabi that contains what all of the samples are\n",
    "        Esentially what we want to do is quite simple, however, we will need to be careful with the colouring\n",
    "        As there are going to be a shed ton of sequences we will probably want to order the sequences found some how\n",
    "        and colour those, then maybe colour sequences found at below 5% to grey. Something like that.\n",
    "        I have a load of code from previous work when we were trying to look for actual SymPortal ITS2 type profiles\n",
    "        within the env samples but we are no longer doing that. Still some of the code below might be useful.\n",
    "        Actually I'm going to drop it into a junk code def and start from scratch here.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        sp_output_df = pickle.load(open('sp_output_df_all_dates.pickle', 'rb'))\n",
    "        QC_info_df= pickle.load(open('QC_info_df_all_dates.pickle', 'rb'))\n",
    "        info_df = pickle.load(open('info_df_all_dates.pickle', 'rb'))\n",
    "    except:\n",
    "        # read in the SymPortal output\n",
    "        sp_output_df = pd.read_csv('131_142.DIVs.absolute.txt', sep='\\t', lineterminator='\\n')\n",
    "\n",
    "        # The SP output contains the QC info columns between the DIVs and the no_name ITS2 columns.\n",
    "        # lets put the QC info columns into a seperate df.\n",
    "        QC_info_df = sp_output_df[['Samples','raw_contigs', 'post_qc_absolute_seqs', 'post_qc_unique_seqs',\n",
    "                                'post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs',\n",
    "                                'post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs',\n",
    "                                   'size_screening_violation_absolute', 'size_screening_violation_unique',\n",
    "                                   'post_med_absolute', 'post_med_unique']]\n",
    "\n",
    "        # now lets drop the QC columns from the SP output df and also drop the clade summation columns\n",
    "        # we will be left with just clumns for each one of the sequences found in the samples\n",
    "        sp_output_df.drop(columns=['noName Clade A', 'noName Clade B', 'noName Clade C', 'noName Clade D',\n",
    "                                'noName Clade E', 'noName Clade F', 'noName Clade G', 'noName Clade H',\n",
    "                                'noName Clade I', 'raw_contigs', 'post_qc_absolute_seqs', 'post_qc_unique_seqs',\n",
    "                                'post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs',\n",
    "                                'post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs',\n",
    "                                   'size_screening_violation_absolute', 'size_screening_violation_unique',\n",
    "                                   'post_med_absolute', 'post_med_unique'\n",
    "                                   ]\n",
    "                          , inplace=True)\n",
    "\n",
    "        # read in the info file\n",
    "        info_df = pd.read_csv('info_300718.csv')\n",
    "\n",
    "        # we no longer drop the dates\n",
    "        # drop the rows that aren't from the 22.08.2016 data\n",
    "        info_df = info_df[info_df['coral genus'] != 'Sponge']\n",
    "        # need to use the ampersand rather than 'and': https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o\n",
    "        info_df = info_df[(info_df['Sample no.'] != 'negative extration') & (info_df['Sample no.'] != 'negative pcr') & (info_df['Sample no.'] != 'DIV_accession')]\n",
    "\n",
    "        # Now we need to link the SP output to the sample names in the excel. Annoyingly they are formatted\n",
    "        # slightly differently so we can't make a direct comparison.\n",
    "        # easiest way to link them is to see if the first part of the SP name is the same as the first part\n",
    "        # of the 'sequence file' in the meta info\n",
    "        # when doing this we can also drop the SP info for those samples that won't be used i.e. those that\n",
    "        # aren't now in the info_df\n",
    "\n",
    "        # firstly rename the colums so that they are 'sample_name' in all of the dfs\n",
    "        QC_info_df.rename(index=str, columns={'Samples': 'sample_name'}, inplace=True)\n",
    "        sp_output_df.rename(index=str, columns={'Samples': 'sample_name'}, inplace=True)\n",
    "        info_df.rename(index=str, columns={'Sample Name': 'sample_name'}, inplace=True)\n",
    "\n",
    "        indices_to_drop = []\n",
    "        for sp_index in sp_output_df.index.values.tolist():\n",
    "            # keep track of whether the sp_index was found in the info table\n",
    "            # if it wasn't then it should be dropped\n",
    "            sys.stdout.write('\\rsp_index: {}'.format(sp_index))\n",
    "            found = False\n",
    "            for info_index in info_df.index.values.tolist():\n",
    "                if sp_output_df.loc[sp_index, 'sample_name'].split('_')[0] == info_df.loc[info_index, 'Sequence file'].split('_')[0]:\n",
    "                    found = True\n",
    "                    # then these are a related set of rows and we should make the sample_names the same\n",
    "                    sp_output_df.loc[sp_index, 'sample_name'] = info_df.loc[info_index, 'sample_name']\n",
    "                    QC_info_df.loc[sp_index, 'sample_name'] = info_df.loc[info_index, 'sample_name']\n",
    "\n",
    "\n",
    "            if not found:\n",
    "                indices_to_drop.append(sp_index)\n",
    "\n",
    "        # drop the rows from the SP output tables that aren't going to be used\n",
    "        sp_output_df.drop(inplace=True, index=indices_to_drop)\n",
    "        QC_info_df.drop(inplace=True, index=indices_to_drop)\n",
    "\n",
    "        # let's sort out the 'environ type' column in the info_df\n",
    "        # currently it is a bit of a mess\n",
    "        for index in info_df.index.values.tolist():\n",
    "            if 'coral' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'coral'\n",
    "            elif 'seawater' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'sea_water'\n",
    "            elif 'mucus' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'mucus'\n",
    "            elif 'SA' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'sed_close'\n",
    "            elif 'SB' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'sed_far'\n",
    "            elif 'TA' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'turf'\n",
    "\n",
    "        # now clean up the df indices\n",
    "        info_df.index = range(len(info_df))\n",
    "        sp_output_df.index = range(len(sp_output_df))\n",
    "        QC_info_df.index = range(len(QC_info_df))\n",
    "\n",
    "        # make the sample_name column the index for each of the datasets\n",
    "        info_df.set_index('sample_name', inplace=True)\n",
    "        sp_output_df.set_index('sample_name', inplace=True)\n",
    "        QC_info_df.set_index('sample_name', inplace=True)\n",
    "\n",
    "        # pickle the out put and put a check in place to see if we need to do the above\n",
    "        pickle.dump(sp_output_df, open('sp_output_df_all_dates.pickle', 'wb'))\n",
    "        pickle.dump(QC_info_df, open('QC_info_df_all_dates.pickle', 'wb'))\n",
    "        pickle.dump(info_df, open('info_df_all_dates.pickle', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "    # so we want to plot the ITS2 sequence diversity in each of the samples as bar charts\n",
    "    # We are going to have a huge diversity of sequences to deal with so I think something along the lines\n",
    "    # of plotting the top n most abundant sequences. The term 'most abundant' should be considered carefully here\n",
    "    # I think it will be best if we work on a sample by sample basis. i.e. we pick the n sequences that have the\n",
    "    # highest representation in any one sample. So for example what we are not doing is seeing how many times\n",
    "    # C3 was sequenced across all of the samples, and finding that it is a lot and therefore plotting it.\n",
    "    # we are looking in each of the samples and seeing the highest proportion it is found at in any one sample.\n",
    "    # This way we should have the best chance of having a coloured representation for each sample's most abundant\n",
    "    # sequence.\n",
    "\n",
    "    # to start lets go sample by sample and see what the highest prop for each seq is.\n",
    "\n",
    "    # dict to hold info on which sample and what the proportion is for each sequence\n",
    "    # key = sequence name, value = tup ( sample name, relative abundance)\n",
    "    try:\n",
    "        seq_rel_abund_calculator_dict = pickle.load(open('seq_rel_abund_calculator_dict_all_dates.pickle', 'rb'))\n",
    "    except:\n",
    "        seq_rel_abund_calculator_dict = {}\n",
    "        for sample_index in sp_output_df.index.values.tolist():\n",
    "            sys.stdout.write('\\nGeting rel seq abundances from {}\\n'.format(sample_index))\n",
    "            # temp_prop_array = sp_output_df.loc[sample_index].div(sp_output_df.loc[sample_index].sum(axis='index'))\n",
    "            temp_prop_array = sp_output_df.loc[sample_index].div(sp_output_df.loc[sample_index].sum())\n",
    "            for seq_name in temp_prop_array.keys():\n",
    "                sys.stdout.write('\\rseq: {}'.format(seq_name))\n",
    "                val = temp_prop_array[seq_name]\n",
    "                if val != 0:  # if the sequences was found in the sample\n",
    "                    # If the sequence is already in the dict\n",
    "                    if seq_name in seq_rel_abund_calculator_dict.keys():\n",
    "                        # check to seee if the rel abundance is larger than the one already logged\n",
    "                        if val > seq_rel_abund_calculator_dict[seq_name][1]:\n",
    "                            seq_rel_abund_calculator_dict[seq_name] = (sample_index, val)\n",
    "                    # if we haven't logged for this sequence yet, then add this as the first log\n",
    "                    else:\n",
    "                        seq_rel_abund_calculator_dict[seq_name] = (sample_index, val)\n",
    "        pickle.dump(seq_rel_abund_calculator_dict, open('seq_rel_abund_calculator_dict_all_dates.pickle', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "    # here we have a dict that contains the largest rel_abundances per sample for each of the seqs\n",
    "    # now we can sort this to look at the top ? n sequences to start with (I'm not sure how the colouring will\n",
    "    # look like so lets just start with 30 and see where we get to)\n",
    "    sorted_list = sorted(seq_rel_abund_calculator_dict.items(), key = lambda x: x[1][1], reverse=True)\n",
    "    most_abund_seq_names = [tup[0] for tup in sorted_list]\n",
    "\n",
    "    # from the above sorted list we can then plot these sequences with colour and all others with grey scale\n",
    "    # lets make a coulour dictionary for the most common types\n",
    "    colour_list = get_colour_list()\n",
    "    colour_dict = {}\n",
    "    num_coloured_seqs = 30\n",
    "\n",
    "    # we will also need a grey palette for those sequences that are not the ones being annotated\n",
    "    grey_palette = ['#D0CFD4', '#89888D', '#4A4A4C', '#8A8C82', '#D4D5D0', '#53544F']\n",
    "\n",
    "    # make the dict\n",
    "    for i in range(len(most_abund_seq_names)):\n",
    "        if i < num_coloured_seqs:\n",
    "            colour_dict[most_abund_seq_names[i]] = colour_list[i]\n",
    "        else:\n",
    "            colour_dict[most_abund_seq_names[i]] = grey_palette[i % 6]\n",
    "\n",
    "    # add the 'low' key and assign it to grey for later on\n",
    "    # the low category will be created later on to hold the grouped abundances of sequences in samples\n",
    "    # below a certain rel abund cutoff\n",
    "    colour_dict['low'] = '#D0CFD4'\n",
    "\n",
    "    # for plotting we will also need to collect the 'rectangle' objects from the plotting process to use to make\n",
    "    # the legend which we will display on the plot right at the end\n",
    "    # this is going to be a little tricky to collect as not every sample type group that we are plotting\n",
    "    # is going to have all of the top n sequences. So, we will have to pick up rectangles when they come up in\n",
    "    # the plotting.\n",
    "    # to keep track of which sequences we need we have:\n",
    "    top_n_seqs_to_get = most_abund_seq_names[:num_coloured_seqs]\n",
    "    # to keep track of which sequences we have already collected we have:\n",
    "    top_n_seqs_done = []\n",
    "    #finally to collect the rectangles and store them in order despite collecting them out of order we have:\n",
    "    legend_rectangle_holder = [[] for i in range(num_coloured_seqs)]\n",
    "\n",
    "\n",
    "    # set up the plotting environment\n",
    "    # one plot for coral, mucus, seawater, sediment, turf_algae\n",
    "    # f, axarr = plt.subplots(4, 5)\n",
    "    # # counter to reference which set of axes we are plotting on\n",
    "    # axarr_index = 0\n",
    "\n",
    "    # rather than work with the subplot standard layout we will use subplot2grid for our set up and have some hard\n",
    "    # coded axes\n",
    "    # ax1 = (plt.subplot2grid((6, 5), (0, 0), colspan=3), 'coral', '22.08.2016')\n",
    "    # ax2 = (plt.subplot2grid((6, 5), (0, 3), colspan=2), 'coral', '06.09.2016')\n",
    "    # ax3 = (plt.subplot2grid((6, 5), (1, 0), colspan=3), 'mucus', '22.08.2016')\n",
    "\n",
    "\n",
    "    ax_list = [\n",
    "        # coral\n",
    "        (plt.subplot2grid((6, 70), (0, 0), colspan=50), 'coral', '22.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (0, 53), colspan=17), 'coral', '06.09.2016'),\n",
    "        # mucus\n",
    "        (plt.subplot2grid((6, 70), (1, 0), colspan=49), 'mucus', '22.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (1, 53), colspan=17), 'mucus', '06.09.2016'),\n",
    "        # sea water\n",
    "        (plt.subplot2grid((6, 70), (2, 0 ),  colspan=5), 'sea_water', '22.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (2, 13),  colspan=3), 'sea_water', '31.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (2, 22), colspan=2), 'sea_water', '01.09.2016'),\n",
    "        (plt.subplot2grid((6, 70), (2, 29), colspan=5), 'sea_water', '06.09.2016'),\n",
    "        (plt.subplot2grid((6, 70), (2, 40), colspan=5), 'sea_water', '02.10.2016'),\n",
    "        # sed_close\n",
    "        (plt.subplot2grid((6, 70), (3, 0 ), colspan=5), 'sed_close', '22.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (3, 13), colspan=3), 'sed_close', '31.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (3, 22), colspan=2), 'sed_close', '01.09.2016'),\n",
    "        (plt.subplot2grid((6, 70), (3, 29), colspan=5), 'sed_close', '06.09.2016'),\n",
    "        (plt.subplot2grid((6, 70), (3, 40), colspan=5), 'sed_close', '02.10.2016'),\n",
    "        # sed_far,\n",
    "        (plt.subplot2grid((6, 70), (4, 0 ), colspan=5), 'sed_far', '22.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (4, 13), colspan=3), 'sed_far', '31.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (4, 22), colspan=2), 'sed_far', '01.09.2016'),\n",
    "        (plt.subplot2grid((6, 70), (4, 29), colspan=5), 'sed_far', '06.09.2016'),\n",
    "        (plt.subplot2grid((6, 70), (4, 40), colspan=5), 'sed_far', '02.10.2016'),\n",
    "        # turf,\n",
    "        (plt.subplot2grid((6, 70), (5, 0 ), colspan= 10), 'turf', '22.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (5, 13), colspan=6 ), 'turf', '31.08.2016'),\n",
    "        (plt.subplot2grid((6, 70), (5, 22), colspan=4 ), 'turf', '01.09.2016'),\n",
    "        (plt.subplot2grid((6, 70), (5, 29), colspan=8 ), 'turf', '06.09.2016'),\n",
    "        (plt.subplot2grid((6, 70), (5, 40), colspan=9 ), 'turf', '02.10.2016')\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # now we need to get the actual plotting information for each of the samples.\n",
    "    # we can do this sample type by sample type\n",
    "    # we will create a local dataframe that will be a sub set of the main sp output dataframe but will\n",
    "    # only contain the samples of the given sample type. It will also eventually only contain the sequence\n",
    "    # information for the sample type in question. Normally I would make a 2D list to hold the plotting\n",
    "    # information but in this case having all of the information in a dataframe is working really well and\n",
    "    # this is how I will do it in future.\n",
    "\n",
    "    # go environment type by environment type\n",
    "    # env_type_list = ['coral', 'mucus', 'sea_water', 'sed_close', 'sed_far', 'turf']\n",
    "    # env_type_list = ['sea_water', 'sed_close', 'sed_far', 'turf']\n",
    "    # for env_type in env_type_list:\n",
    "    #     # then go date by date\n",
    "    #     date_list = ['22.08.2016', '31.08.2016', '01.09.2016', '06.09.2016', '02.10.2016']\n",
    "    #     for date in date_list:\n",
    "    for ax_item in ax_list:\n",
    "        try:\n",
    "            env_sp_output_df_prop = pickle.load(open('env_sp_output_df_prop_{}_{}.pickle'.format(ax_item[1], ax_item[2]), 'rb'))\n",
    "            sorted_list_of_env_specific_seqs = pickle.load(open('sorted_list_of_env_specific_seqs_{}_{}.pickle'.format(ax_item[1], ax_item[2]), 'rb'))\n",
    "        except:\n",
    "            sys.stdout.write('\\nGenerating plotting info for {} samples {}\\n'.format(ax_item[1], ax_item[2]))\n",
    "            # currently we have something like 4000 sequences to plot which is too may\n",
    "            # I think it will be much easier if we group the sequences that are found below a certain\n",
    "            # threshold. I think the best way to do this is to create a slice of the main df that\n",
    "            # contain the information for the samples of the env_type only\n",
    "\n",
    "            # get subset of the main dfs that contain only the env_type samples from the given date\n",
    "            env_info_df = info_df[(info_df['environ type'] == ax_item[1]) & (info_df['date collected'] == ax_item[2])]\n",
    "\n",
    "            # we need to drop any samples that have only zeros in their columns\n",
    "            # https://stackoverflow.com/questions/22649693/drop-rows-with-all-zeros-in-pandas-data-frame\n",
    "            env_info_df = env_info_df[(env_info_df.T != 0).any()]\n",
    "\n",
    "\n",
    "            env_sp_output_df = sp_output_df.loc[env_info_df.index.values.tolist()]\n",
    "\n",
    "            # we need to drop any samples that have only zeros in their columns\n",
    "            # https://stackoverflow.com/questions/22649693/drop-rows-with-all-zeros-in-pandas-data-frame\n",
    "            env_sp_output_df = env_sp_output_df[(env_sp_output_df.T != 0).any()]\n",
    "\n",
    "            # append a 'low' columns to the env_sp_ouput_df and populate with 0s\n",
    "            env_sp_output_df['low'] = 0\n",
    "            # now make a proportions version of the df, rather than absolute counts\n",
    "            env_sp_output_df_prop = env_sp_output_df[:].div(env_sp_output_df[:].sum(axis=1), axis=0)\n",
    "\n",
    "            # now as we work our way through we can sum up the low sequences into this column\n",
    "            # we can then check for 0 columns and drop these.\n",
    "\n",
    "            # get a list of the sequences found in the collection of samples of the given type and order\n",
    "            # them according to summed rel_abundance acorss all samples. This should be the order in which\n",
    "            # the samples are plotted\n",
    "            # at the same time we can get the info we need for plotting\n",
    "\n",
    "            summed_seq_rel_abund_across_smpls_dict = {seq: 0 for seq in list(sp_output_df)}\n",
    "\n",
    "            # we are also going to need to put the samples in an order that makes sense.\n",
    "            # Ideally I would like to take the time to do a proper travelling salesman analysis\n",
    "            # and I was thinking of putting in an ant colony system to do this but...\n",
    "            # I'm not sure we've got time for that. So let's do something a little more simple which\n",
    "            # should still be effective and look groovy\n",
    "            # Lets sort according to the majority sequence. I.e. lets put the samples into groups that\n",
    "            # are defined by what their majorty sequence is, then lets plot in order of those groups.\n",
    "            # within the groups we will plot in the order of the most abundant rel abund first.\n",
    "            # to get this information we will have a dict to collect it. The key of the dict will\n",
    "            # be the majority sequence with the value being a list that contains tuples. One tuple\n",
    "            # for each sample in the list which will contain the sample_name and the re abund of the maj seq\n",
    "\n",
    "            # Dict for holding the sample_sorting info\n",
    "            sample_sorting_info_dict = defaultdict(list)\n",
    "\n",
    "            for sp_index in env_sp_output_df_prop.index.values.tolist():\n",
    "                sys.stdout.write('\\r{}'.format(sp_index))\n",
    "                # we need to get the name of the most abundant sequence and its rel abund for each sample\n",
    "                # for the sample_sorting_info_dict\n",
    "                most_abund_seq_name = env_sp_output_df_prop.loc[sp_index].idxmax(axis='index')\n",
    "\n",
    "                rel_abund_of_most_abund_seq = env_sp_output_df_prop.loc[sp_index, most_abund_seq_name]\n",
    "\n",
    "                apples = 'asdf'\n",
    "                sample_sorting_info_dict[most_abund_seq_name].append((sp_index, rel_abund_of_most_abund_seq))\n",
    "                # Put its seq rel abundances ino the summed_seq_rel_abund_across... dict\n",
    "\n",
    "                for non_zero_seq in env_sp_output_df_prop.loc[sp_index][env_sp_output_df_prop.loc[sp_index] > 0].index:  # not including the final 'low' columns (this will be zero)\n",
    "                    val = env_sp_output_df_prop.loc[sp_index, non_zero_seq]\n",
    "                    # be sure to count the value of this cell and using it in judging which are the most\n",
    "                    # abundant sequences before we check whether to relegate it to the 'low' column\n",
    "                    summed_seq_rel_abund_across_smpls_dict[non_zero_seq] += val\n",
    "                    if val < 0.005:\n",
    "                        env_sp_output_df_prop.loc[sp_index, 'low'] += val\n",
    "                        env_sp_output_df_prop.loc[sp_index, non_zero_seq] = 0\n",
    "\n",
    "\n",
    "            # here we can get a sorted sample list using the sample_sorting_info_dict\n",
    "            sorted_sample_list = []\n",
    "            # we want to work through the sample_sorting_info_dict by the longest lists first\n",
    "            sorted_keys_for_sample_sort = \\\n",
    "                [tup[0] for tup in sorted(sample_sorting_info_dict.items(), key=lambda x: len(x[1]), reverse=True)]\n",
    "            # for each of teh maj seq groups\n",
    "            for sorted_key in sorted_keys_for_sample_sort:\n",
    "                # now within each of these lists we want to order according to the rel_abundance of the sequences\n",
    "                sorted_list_of_samples_of_group = [tup[0] for tup in sorted(sample_sorting_info_dict[sorted_key], key=lambda x: x[1], reverse=True)]\n",
    "                sorted_sample_list.extend(sorted_list_of_samples_of_group)\n",
    "\n",
    "            # now we should re-order the df so that it is in the sample order of sorted_sample_list\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop.reindex(sorted_sample_list)\n",
    "\n",
    "            # here we have a dict that contains the abundances of the seqs for the coral samples\n",
    "            # we will plot the coral samples' seqs in the order of the sequences in this dict\n",
    "            sorted_list_of_env_specific_seqs_tup \\\n",
    "                = sorted(summed_seq_rel_abund_across_smpls_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            sorted_list_of_env_specific_seqs = [tup[0] for tup in sorted_list_of_env_specific_seqs_tup]\n",
    "\n",
    "            # Now we check for zero only columns and drop them\n",
    "            # we also need to remove these columns from the sorted\n",
    "            non_zero_cols = list(env_sp_output_df_prop.loc[:, (env_sp_output_df_prop != 0).any(axis=0)])\n",
    "            sorted_list_of_env_specific_seqs = [seq for seq in sorted_list_of_env_specific_seqs if seq in non_zero_cols]\n",
    "\n",
    "            # add the 'low' column only if the 'low' column is a non-zero column, i.e. used\n",
    "            if 'low' in non_zero_cols:\n",
    "                sorted_list_of_env_specific_seqs.append('low')\n",
    "\n",
    "            # now drop the cols\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop[non_zero_cols]\n",
    "\n",
    "            # we also have the plotting list which contains the info we will be plotting.\n",
    "\n",
    "            # the plotting_list is currently organised in a different order to that of the sorted_list_of_env...\n",
    "            # we need to change this order\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop[sorted_list_of_env_specific_seqs]\n",
    "\n",
    "            # we now need to transpose this\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop.transpose()\n",
    "\n",
    "            # pickle the info we've been plotting\n",
    "            pickle.dump(env_sp_output_df_prop, open('env_sp_output_df_prop_{}_{}.pickle'.format(ax_item[1], ax_item[2]), 'wb'))\n",
    "            pickle.dump(sorted_list_of_env_specific_seqs, open('sorted_list_of_env_specific_seqs_{}_{}.pickle'.format(ax_item[1], ax_item[2]), 'wb'))\n",
    "\n",
    "        # here we finally have the plotting lists in the order of the sorted_list_of_env\n",
    "        bottom = [0 for smp in list(env_sp_output_df_prop)]\n",
    "        bar_list = []\n",
    "        # for each sample\n",
    "        plotting_indices = range(len(list(env_sp_output_df_prop)))\n",
    "\n",
    "        # for each sequence\n",
    "        list_of_seqs = env_sp_output_df_prop.index.values.tolist()\n",
    "        for i in range(len(list_of_seqs)):\n",
    "\n",
    "            sys.stdout.write('\\r{}/{}'.format(i, len(list(env_sp_output_df_prop.iloc[:, 0]))))\n",
    "            bar_list.append(ax_item[0].bar(plotting_indices, list(env_sp_output_df_prop.iloc[i]), 1, bottom,\n",
    "                                                   color=colour_dict[sorted_list_of_env_specific_seqs[i]]))\n",
    "            bottom = [L + M for L, M in zip(bottom, list(env_sp_output_df_prop.iloc[i]))]\n",
    "\n",
    "            # check to see if the seq we are plotting is still in the top_n_seqs list\n",
    "            # if it is in this list then we still need to grab a rectangle for plotting\n",
    "            # the legend. Once we have grabbed the rectangle then we should remove the seq from the top_n_seqs list\n",
    "            seq_name = list_of_seqs[i]\n",
    "\n",
    "            if seq_name in top_n_seqs_to_get and seq_name not in top_n_seqs_done:\n",
    "                # then this is a seq that we still need to get a rectangle for the legend\n",
    "                legend_rectangle_holder[top_n_seqs_to_get.index(seq_name)].append(bar_list[-1][0])\n",
    "                top_n_seqs_done.append(seq_name)\n",
    "\n",
    "        # https://stackoverflow.com/questions/12998430/remove-xticks-in-a-matplotlib-plot\n",
    "        ax_item[0].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "        # https://stackoverflow.com/questions/15858192/how-to-set-xlim-and-ylim-for-a-subplot-in-matplotlib\n",
    "        ax_item[0].set_xlim((-0.5, (len(list(env_sp_output_df_prop)) - 0.5)))\n",
    "        # https://stackoverflow.com/questions/925024/how-can-i-remove-the-top-and-right-axis-in-matplotlib\n",
    "        ax_item[0].spines['right'].set_visible(False)\n",
    "        ax_item[0].spines['top'].set_visible(False)\n",
    "\n",
    "        # To make the legend for this mo fo is going to be a little tricky. The legend argument basically takes\n",
    "        # two lists. The first list should contain a rectangle object for each of the sequences (this\n",
    "        # will be the coloured box). We get this object from the bar objects that we are creating.\n",
    "        # The second list is a list of the labels. This should be easier. We can just use the\n",
    "        # most_abund_seq_names[:30] for these.\n",
    "        # to grab the rectangles, I think its best to pick them up during the plotting and hold them in a list\n",
    "        # outside of the subplot loop. We will need a holder for the objects to populate.\n",
    "\n",
    "        #https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_xticks.html#matplotlib.axes.Axes.set_xticks\n",
    "        ax_item[0].set_yticks([1], minor=False)\n",
    "        #https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html#matplotlib.axes.Axes.set_xticklabels\n",
    "        ax_item[0].set_yticklabels(['1'])\n",
    "        if ax_item[2] == '22.08.2016':\n",
    "            ax_item[0].set_ylabel(ax_item[1])\n",
    "\n",
    "        # only if we're on the final plot then we should add the date ticks\n",
    "        if ax_item[1] == 'turf':\n",
    "            ax_item[0].set_xlabel(ax_item[2])\n",
    "        # also if we are on the mucus we should put the labels for the dates on the axis\n",
    "        if ax_item[1] == 'mucus':\n",
    "            ax_item[0].set_xlabel(ax_item[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #https://stackoverflow.com/questions/16150819/common-xlabel-ylabel-for-matplotlib-subplots/26892326\n",
    "    plt.text(0, 0.5, 'ITS2 sequence relative abundance', va='center', rotation='vertical')\n",
    "    ordered_rectange_list = [sub_list[0] for sub_list in legend_rectangle_holder]\n",
    "    plt.legend(ordered_rectange_list, top_n_seqs_to_get, loc='lower center')\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig('env_smp_diversity_bars_all_dates.svg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversity bars all dates grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diverstiy_figs_all_dates_grouped_dates():\n",
    "    '''\n",
    "    This is yet another development in which we will group all of the dates together rather than sorting them\n",
    "    individually.\n",
    "    This is a development from the above code. Here in this code we will plot all of the date replicates\n",
    "    for the env_samples as these could in theory aid a little bit in the diversity comparison.\n",
    "    I will put these on the same axis as the samples of the same type with a gap or so inbetween.\n",
    "    This will be the code to create the diversity figure for Gabi's paper.\n",
    "        We have used symportal to do the QC work on the samples.\n",
    "        As such we have tab delimited outputs that we can read into pandas dataframes\n",
    "        We also have the info list from Gabi that contains what all of the samples are\n",
    "        Esentially what we want to do is quite simple, however, we will need to be careful with the colouring\n",
    "        As there are going to be a shed ton of sequences we will probably want to order the sequences found some how\n",
    "        and colour those, then maybe colour sequences found at below 5% to grey. Something like that.\n",
    "        I have a load of code from previous work when we were trying to look for actual SymPortal ITS2 type profiles\n",
    "        within the env samples but we are no longer doing that. Still some of the code below might be useful.\n",
    "        Actually I'm going to drop it into a junk code def and start from scratch here.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        sp_output_df = pickle.load(open('sp_output_df_all_dates.pickle', 'rb'))\n",
    "        QC_info_df= pickle.load(open('QC_info_df_all_dates.pickle', 'rb'))\n",
    "        info_df = pickle.load(open('info_df_all_dates.pickle', 'rb'))\n",
    "    except:\n",
    "        # read in the SymPortal output\n",
    "        sp_output_df = pd.read_csv('131_142.DIVs.absolute.txt', sep='\\t', lineterminator='\\n')\n",
    "\n",
    "        # The SP output contains the QC info columns between the DIVs and the no_name ITS2 columns.\n",
    "        # lets put the QC info columns into a seperate df.\n",
    "        QC_info_df = sp_output_df[['Samples','raw_contigs', 'post_qc_absolute_seqs', 'post_qc_unique_seqs',\n",
    "                                'post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs',\n",
    "                                'post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs',\n",
    "                                   'size_screening_violation_absolute', 'size_screening_violation_unique',\n",
    "                                   'post_med_absolute', 'post_med_unique']]\n",
    "\n",
    "        # now lets drop the QC columns from the SP output df and also drop the clade summation columns\n",
    "        # we will be left with just clumns for each one of the sequences found in the samples\n",
    "        sp_output_df.drop(columns=['noName Clade A', 'noName Clade B', 'noName Clade C', 'noName Clade D',\n",
    "                                'noName Clade E', 'noName Clade F', 'noName Clade G', 'noName Clade H',\n",
    "                                'noName Clade I', 'raw_contigs', 'post_qc_absolute_seqs', 'post_qc_unique_seqs',\n",
    "                                'post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs',\n",
    "                                'post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs',\n",
    "                                   'size_screening_violation_absolute', 'size_screening_violation_unique',\n",
    "                                   'post_med_absolute', 'post_med_unique'\n",
    "                                   ]\n",
    "                          , inplace=True)\n",
    "\n",
    "        # read in the info file\n",
    "        info_df = pd.read_csv('info_300718.csv')\n",
    "\n",
    "        # we no longer drop the dates\n",
    "        # drop the rows that aren't from the 22.08.2016 data\n",
    "        info_df = info_df[info_df['coral genus'] != 'Sponge']\n",
    "        # need to use the ampersand rather than 'and': https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o\n",
    "        info_df = info_df[(info_df['Sample no.'] != 'negative extration') & (info_df['Sample no.'] != 'negative pcr') & (info_df['Sample no.'] != 'DIV_accession')]\n",
    "\n",
    "        # Now we need to link the SP output to the sample names in the excel. Annoyingly they are formatted\n",
    "        # slightly differently so we can't make a direct comparison.\n",
    "        # easiest way to link them is to see if the first part of the SP name is the same as the first part\n",
    "        # of the 'sequence file' in the meta info\n",
    "        # when doing this we can also drop the SP info for those samples that won't be used i.e. those that\n",
    "        # aren't now in the info_df\n",
    "\n",
    "        # firstly rename the colums so that they are 'sample_name' in all of the dfs\n",
    "        QC_info_df.rename(index=str, columns={'Samples': 'sample_name'}, inplace=True)\n",
    "        sp_output_df.rename(index=str, columns={'Samples': 'sample_name'}, inplace=True)\n",
    "        info_df.rename(index=str, columns={'Sample Name': 'sample_name'}, inplace=True)\n",
    "\n",
    "        indices_to_drop = []\n",
    "        for sp_index in sp_output_df.index.values.tolist():\n",
    "            # keep track of whether the sp_index was found in the info table\n",
    "            # if it wasn't then it should be dropped\n",
    "            sys.stdout.write('\\rsp_index: {}'.format(sp_index))\n",
    "            found = False\n",
    "            for info_index in info_df.index.values.tolist():\n",
    "                if sp_output_df.loc[sp_index, 'sample_name'].split('_')[0] == info_df.loc[info_index, 'Sequence file'].split('_')[0]:\n",
    "                    found = True\n",
    "                    # then these are a related set of rows and we should make the sample_names the same\n",
    "                    sp_output_df.loc[sp_index, 'sample_name'] = info_df.loc[info_index, 'sample_name']\n",
    "                    QC_info_df.loc[sp_index, 'sample_name'] = info_df.loc[info_index, 'sample_name']\n",
    "\n",
    "\n",
    "            if not found:\n",
    "                indices_to_drop.append(sp_index)\n",
    "\n",
    "        # drop the rows from the SP output tables that aren't going to be used\n",
    "        sp_output_df.drop(inplace=True, index=indices_to_drop)\n",
    "        QC_info_df.drop(inplace=True, index=indices_to_drop)\n",
    "\n",
    "        # let's sort out the 'environ type' column in the info_df\n",
    "        # currently it is a bit of a mess\n",
    "        for index in info_df.index.values.tolist():\n",
    "            if 'coral' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'coral'\n",
    "            elif 'seawater' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'sea_water'\n",
    "            elif 'mucus' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'mucus'\n",
    "            elif 'SA' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'sed_close'\n",
    "            elif 'SB' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'sed_far'\n",
    "            elif 'TA' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'turf'\n",
    "\n",
    "        # now clean up the df indices\n",
    "        info_df.index = range(len(info_df))\n",
    "        sp_output_df.index = range(len(sp_output_df))\n",
    "        QC_info_df.index = range(len(QC_info_df))\n",
    "\n",
    "        # make the sample_name column the index for each of the datasets\n",
    "        info_df.set_index('sample_name', inplace=True)\n",
    "        sp_output_df.set_index('sample_name', inplace=True)\n",
    "        QC_info_df.set_index('sample_name', inplace=True)\n",
    "\n",
    "        # pickle the out put and put a check in place to see if we need to do the above\n",
    "        pickle.dump(sp_output_df, open('sp_output_df_all_dates.pickle', 'wb'))\n",
    "        pickle.dump(QC_info_df, open('QC_info_df_all_dates.pickle', 'wb'))\n",
    "        pickle.dump(info_df, open('info_df_all_dates.pickle', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "    # so we want to plot the ITS2 sequence diversity in each of the samples as bar charts\n",
    "    # We are going to have a huge diversity of sequences to deal with so I think something along the lines\n",
    "    # of plotting the top n most abundant sequences. The term 'most abundant' should be considered carefully here\n",
    "    # I think it will be best if we work on a sample by sample basis. i.e. we pick the n sequences that have the\n",
    "    # highest representation in any one sample. So for example what we are not doing is seeing how many times\n",
    "    # C3 was sequenced across all of the samples, and finding that it is a lot and therefore plotting it.\n",
    "    # we are looking in each of the samples and seeing the highest proportion it is found at in any one sample.\n",
    "    # This way we should have the best chance of having a coloured representation for each sample's most abundant\n",
    "    # sequence.\n",
    "\n",
    "    # to start lets go sample by sample and see what the highest prop for each seq is.\n",
    "\n",
    "    # dict to hold info on which sample and what the proportion is for each sequence\n",
    "    # key = sequence name, value = tup ( sample name, relative abundance)\n",
    "    try:\n",
    "        seq_rel_abund_calculator_dict = pickle.load(open('seq_rel_abund_calculator_dict_all_dates.pickle', 'rb'))\n",
    "    except:\n",
    "        seq_rel_abund_calculator_dict = {}\n",
    "        for sample_index in sp_output_df.index.values.tolist():\n",
    "            sys.stdout.write('\\nGeting rel seq abundances from {}\\n'.format(sample_index))\n",
    "            # temp_prop_array = sp_output_df.loc[sample_index].div(sp_output_df.loc[sample_index].sum(axis='index'))\n",
    "            temp_prop_array = sp_output_df.loc[sample_index].div(sp_output_df.loc[sample_index].sum())\n",
    "            for seq_name in temp_prop_array.keys():\n",
    "                sys.stdout.write('\\rseq: {}'.format(seq_name))\n",
    "                val = temp_prop_array[seq_name]\n",
    "                if val != 0:  # if the sequences was found in the sample\n",
    "                    # If the sequence is already in the dict\n",
    "                    if seq_name in seq_rel_abund_calculator_dict.keys():\n",
    "                        # check to seee if the rel abundance is larger than the one already logged\n",
    "                        if val > seq_rel_abund_calculator_dict[seq_name][1]:\n",
    "                            seq_rel_abund_calculator_dict[seq_name] = (sample_index, val)\n",
    "                    # if we haven't logged for this sequence yet, then add this as the first log\n",
    "                    else:\n",
    "                        seq_rel_abund_calculator_dict[seq_name] = (sample_index, val)\n",
    "        pickle.dump(seq_rel_abund_calculator_dict, open('seq_rel_abund_calculator_dict_all_dates.pickle', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "    # here we have a dict that contains the largest rel_abundances per sample for each of the seqs\n",
    "    # now we can sort this to look at the top ? n sequences to start with (I'm not sure how the colouring will\n",
    "    # look like so lets just start with 30 and see where we get to)\n",
    "    sorted_list = sorted(seq_rel_abund_calculator_dict.items(), key = lambda x: x[1][1], reverse=True)\n",
    "    most_abund_seq_names = [tup[0] for tup in sorted_list]\n",
    "\n",
    "    # from the above sorted list we can then plot these sequences with colour and all others with grey scale\n",
    "    # lets make a coulour dictionary for the most common types\n",
    "    colour_list = get_colour_list()\n",
    "    colour_dict = {}\n",
    "    num_coloured_seqs = 30\n",
    "\n",
    "    # we will also need a grey palette for those sequences that are not the ones being annotated\n",
    "    grey_palette = ['#D0CFD4', '#89888D', '#4A4A4C', '#8A8C82', '#D4D5D0', '#53544F']\n",
    "\n",
    "    # make the dict\n",
    "    for i in range(len(most_abund_seq_names)):\n",
    "        if i < num_coloured_seqs:\n",
    "            colour_dict[most_abund_seq_names[i]] = colour_list[i]\n",
    "        else:\n",
    "            colour_dict[most_abund_seq_names[i]] = grey_palette[i % 6]\n",
    "\n",
    "    # add the 'low' key and assign it to grey for later on\n",
    "    # the low category will be created later on to hold the grouped abundances of sequences in samples\n",
    "    # below a certain rel abund cutoff\n",
    "    colour_dict['low'] = '#D0CFD4'\n",
    "\n",
    "    # for plotting we will also need to collect the 'rectangle' objects from the plotting process to use to make\n",
    "    # the legend which we will display on the plot right at the end\n",
    "    # this is going to be a little tricky to collect as not every sample type group that we are plotting\n",
    "    # is going to have all of the top n sequences. So, we will have to pick up rectangles when they come up in\n",
    "    # the plotting.\n",
    "    # to keep track of which sequences we need we have:\n",
    "    top_n_seqs_to_get = most_abund_seq_names[:num_coloured_seqs]\n",
    "    # to keep track of which sequences we have already collected we have:\n",
    "    top_n_seqs_done = []\n",
    "    #finally to collect the rectangles and store them in order despite collecting them out of order we have:\n",
    "    legend_rectangle_holder = [[] for i in range(num_coloured_seqs)]\n",
    "\n",
    "\n",
    "    # set up the plotting environment\n",
    "\n",
    "\n",
    "    # rather than work with the subplot standard layout we will use subplot2grid for our set up and have some hard\n",
    "    # coded axes\n",
    "\n",
    "    this_fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    ax_list = [\n",
    "        # coral\n",
    "        (plt.subplot2grid((6, 67), (0, 0), colspan=67), 'coral'),\n",
    "\n",
    "        # mucus\n",
    "        (plt.subplot2grid((6, 67), (1, 0), colspan=66), 'mucus'),\n",
    "\n",
    "        # sea water\n",
    "        (plt.subplot2grid((6, 67), (2, 0 ),  colspan=20), 'sea_water'),\n",
    "\n",
    "        # sed_close\n",
    "        (plt.subplot2grid((6, 67), (3, 0 ), colspan=20), 'sed_close'),\n",
    "\n",
    "        # sed_far,\n",
    "        (plt.subplot2grid((6, 67), (4, 0 ), colspan=20), 'sed_far'),\n",
    "\n",
    "        # turf,\n",
    "        (plt.subplot2grid((6, 67), (5, 0 ), colspan= 37), 'turf'),\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # now we need to get the actual plotting information for each of the samples.\n",
    "    # we can do this sample type by sample type\n",
    "    # we will create a local dataframe that will be a sub set of the main sp output dataframe but will\n",
    "    # only contain the samples of the given sample type. It will also eventually only contain the sequence\n",
    "    # information for the sample type in question. Normally I would make a 2D list to hold the plotting\n",
    "    # information but in this case having all of the information in a dataframe is working really well and\n",
    "    # this is how I will do it in future.\n",
    "\n",
    "    # go environment type by environment type\n",
    "    # env_type_list = ['coral', 'mucus', 'sea_water', 'sed_close', 'sed_far', 'turf']\n",
    "    # env_type_list = ['sea_water', 'sed_close', 'sed_far', 'turf']\n",
    "    # for env_type in env_type_list:\n",
    "    #     # then go date by date\n",
    "    #     date_list = ['22.08.2016', '31.08.2016', '01.09.2016', '06.09.2016', '02.10.2016']\n",
    "    #     for date in date_list:\n",
    "    for ax_item in ax_list:\n",
    "        try:\n",
    "            env_sp_output_df_prop = pickle.load(open('env_sp_output_df_prop_{}_grouped.pickle'.format(ax_item[1]), 'rb'))\n",
    "            sorted_list_of_env_specific_seqs = pickle.load(open('sorted_list_of_env_specific_seqs_{}_grouped.pickle'.format(ax_item[1]), 'rb'))\n",
    "        except:\n",
    "            sys.stdout.write('\\nGenerating plotting info for samples {}\\n'.format(ax_item[1]))\n",
    "            # currently we have something like 4000 sequences to plot which is too may\n",
    "            # I think it will be much easier if we group the sequences that are found below a certain\n",
    "            # threshold. I think the best way to do this is to create a slice of the main df that\n",
    "            # contain the information for the samples of the env_type only\n",
    "\n",
    "            # get subset of the main dfs that contain only the env_type samples from the given date\n",
    "            env_info_df = info_df[(info_df['environ type'] == ax_item[1])]\n",
    "\n",
    "            # we need to drop any samples that have only zeros in their columns\n",
    "            # https://stackoverflow.com/questions/22649693/drop-rows-with-all-zeros-in-pandas-data-frame\n",
    "            env_info_df = env_info_df[(env_info_df.T != 0).any()]\n",
    "\n",
    "\n",
    "            env_sp_output_df = sp_output_df.loc[env_info_df.index.values.tolist()]\n",
    "\n",
    "            # we need to drop any samples that have only zeros in their columns\n",
    "            # https://stackoverflow.com/questions/22649693/drop-rows-with-all-zeros-in-pandas-data-frame\n",
    "            env_sp_output_df = env_sp_output_df[(env_sp_output_df.T != 0).any()]\n",
    "\n",
    "            # append a 'low' columns to the env_sp_ouput_df and populate with 0s\n",
    "            env_sp_output_df['low'] = 0\n",
    "            # now make a proportions version of the df, rather than absolute counts\n",
    "            env_sp_output_df_prop = env_sp_output_df[:].div(env_sp_output_df[:].sum(axis=1), axis=0)\n",
    "\n",
    "            # now as we work our way through we can sum up the low sequences into this column\n",
    "            # we can then check for 0 columns and drop these.\n",
    "\n",
    "            # get a list of the sequences found in the collection of samples of the given type and order\n",
    "            # them according to summed rel_abundance acorss all samples. This should be the order in which\n",
    "            # the samples are plotted\n",
    "            # at the same time we can get the info we need for plotting\n",
    "\n",
    "            summed_seq_rel_abund_across_smpls_dict = {seq: 0 for seq in list(sp_output_df)}\n",
    "\n",
    "            # we are also going to need to put the samples in an order that makes sense.\n",
    "            # Ideally I would like to take the time to do a proper travelling salesman analysis\n",
    "            # and I was thinking of putting in an ant colony system to do this but...\n",
    "            # I'm not sure we've got time for that. So let's do something a little more simple which\n",
    "            # should still be effective and look groovy\n",
    "            # Lets sort according to the majority sequence. I.e. lets put the samples into groups that\n",
    "            # are defined by what their majorty sequence is, then lets plot in order of those groups.\n",
    "            # within the groups we will plot in the order of the most abundant rel abund first.\n",
    "            # to get this information we will have a dict to collect it. The key of the dict will\n",
    "            # be the majority sequence with the value being a list that contains tuples. One tuple\n",
    "            # for each sample in the list which will contain the sample_name and the re abund of the maj seq\n",
    "\n",
    "            # Dict for holding the sample_sorting info\n",
    "            sample_sorting_info_dict = defaultdict(list)\n",
    "\n",
    "            for sp_index in env_sp_output_df_prop.index.values.tolist():\n",
    "                sys.stdout.write('\\r{}'.format(sp_index))\n",
    "                # we need to get the name of the most abundant sequence and its rel abund for each sample\n",
    "                # for the sample_sorting_info_dict\n",
    "                most_abund_seq_name = env_sp_output_df_prop.loc[sp_index].idxmax(axis='index')\n",
    "\n",
    "                rel_abund_of_most_abund_seq = env_sp_output_df_prop.loc[sp_index, most_abund_seq_name]\n",
    "\n",
    "                apples = 'asdf'\n",
    "                sample_sorting_info_dict[most_abund_seq_name].append((sp_index, rel_abund_of_most_abund_seq))\n",
    "                # Put its seq rel abundances ino the summed_seq_rel_abund_across... dict\n",
    "\n",
    "                for non_zero_seq in env_sp_output_df_prop.loc[sp_index][env_sp_output_df_prop.loc[sp_index] > 0].index:  # not including the final 'low' columns (this will be zero)\n",
    "                    val = env_sp_output_df_prop.loc[sp_index, non_zero_seq]\n",
    "                    # be sure to count the value of this cell and using it in judging which are the most\n",
    "                    # abundant sequences before we check whether to relegate it to the 'low' column\n",
    "                    summed_seq_rel_abund_across_smpls_dict[non_zero_seq] += val\n",
    "                    if val < 0.005:\n",
    "                        env_sp_output_df_prop.loc[sp_index, 'low'] += val\n",
    "                        env_sp_output_df_prop.loc[sp_index, non_zero_seq] = 0\n",
    "\n",
    "\n",
    "            # here we can get a sorted sample list using the sample_sorting_info_dict\n",
    "            sorted_sample_list = []\n",
    "            # we want to work through the sample_sorting_info_dict by the longest lists first\n",
    "            sorted_keys_for_sample_sort = \\\n",
    "                [tup[0] for tup in sorted(sample_sorting_info_dict.items(), key=lambda x: len(x[1]), reverse=True)]\n",
    "            # for each of teh maj seq groups\n",
    "            for sorted_key in sorted_keys_for_sample_sort:\n",
    "                # now within each of these lists we want to order according to the rel_abundance of the sequences\n",
    "                sorted_list_of_samples_of_group = [tup[0] for tup in sorted(sample_sorting_info_dict[sorted_key], key=lambda x: x[1], reverse=True)]\n",
    "                sorted_sample_list.extend(sorted_list_of_samples_of_group)\n",
    "\n",
    "            # now we should re-order the df so that it is in the sample order of sorted_sample_list\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop.reindex(sorted_sample_list)\n",
    "\n",
    "            # here we have a dict that contains the abundances of the seqs for the coral samples\n",
    "            # we will plot the coral samples' seqs in the order of the sequences in this dict\n",
    "            sorted_list_of_env_specific_seqs_tup \\\n",
    "                = sorted(summed_seq_rel_abund_across_smpls_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            sorted_list_of_env_specific_seqs = [tup[0] for tup in sorted_list_of_env_specific_seqs_tup]\n",
    "\n",
    "            # Now we check for zero only columns and drop them\n",
    "            # we also need to remove these columns from the sorted\n",
    "            non_zero_cols = list(env_sp_output_df_prop.loc[:, (env_sp_output_df_prop != 0).any(axis=0)])\n",
    "            sorted_list_of_env_specific_seqs = [seq for seq in sorted_list_of_env_specific_seqs if seq in non_zero_cols]\n",
    "\n",
    "            # add the 'low' column only if the 'low' column is a non-zero column, i.e. used\n",
    "            if 'low' in non_zero_cols:\n",
    "                sorted_list_of_env_specific_seqs.append('low')\n",
    "\n",
    "            # now drop the cols\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop[non_zero_cols]\n",
    "\n",
    "            # we also have the plotting list which contains the info we will be plotting.\n",
    "\n",
    "            # the plotting_list is currently organised in a different order to that of the sorted_list_of_env...\n",
    "            # we need to change this order\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop[sorted_list_of_env_specific_seqs]\n",
    "\n",
    "            # we now need to transpose this\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop.transpose()\n",
    "\n",
    "            # pickle the info we've been plotting\n",
    "            pickle.dump(env_sp_output_df_prop, open('env_sp_output_df_prop_{}_grouped.pickle'.format(ax_item[1]), 'wb'))\n",
    "            pickle.dump(sorted_list_of_env_specific_seqs, open('sorted_list_of_env_specific_seqs_{}_grouped.pickle'.format(ax_item[1]), 'wb'))\n",
    "\n",
    "        # here we finally have the plotting lists in the order of the sorted_list_of_env\n",
    "        bottom = [0 for smp in list(env_sp_output_df_prop)]\n",
    "        bar_list = []\n",
    "        # for each sample\n",
    "        plotting_indices = range(len(list(env_sp_output_df_prop)))\n",
    "\n",
    "        # for each sequence\n",
    "        list_of_seqs = env_sp_output_df_prop.index.values.tolist()\n",
    "        for i in range(len(list_of_seqs)):\n",
    "\n",
    "            sys.stdout.write('\\r{}/{}'.format(i, len(list(env_sp_output_df_prop.iloc[:, 0]))))\n",
    "            bar_list.append(ax_item[0].bar(plotting_indices, list(env_sp_output_df_prop.iloc[i]), 1, bottom,\n",
    "                                                   color=colour_dict[sorted_list_of_env_specific_seqs[i]]))\n",
    "            bottom = [L + M for L, M in zip(bottom, list(env_sp_output_df_prop.iloc[i]))]\n",
    "\n",
    "            # check to see if the seq we are plotting is still in the top_n_seqs list\n",
    "            # if it is in this list then we still need to grab a rectangle for plotting\n",
    "            # the legend. Once we have grabbed the rectangle then we should remove the seq from the top_n_seqs list\n",
    "            seq_name = list_of_seqs[i]\n",
    "\n",
    "            if seq_name in top_n_seqs_to_get and seq_name not in top_n_seqs_done:\n",
    "                # then this is a seq that we still need to get a rectangle for the legend\n",
    "                legend_rectangle_holder[top_n_seqs_to_get.index(seq_name)].append(bar_list[-1][0])\n",
    "                top_n_seqs_done.append(seq_name)\n",
    "\n",
    "        # https://stackoverflow.com/questions/12998430/remove-xticks-in-a-matplotlib-plot\n",
    "        ax_item[0].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "        # https://stackoverflow.com/questions/15858192/how-to-set-xlim-and-ylim-for-a-subplot-in-matplotlib\n",
    "        ax_item[0].set_xlim((-0.5, (len(list(env_sp_output_df_prop)) - 0.5)))\n",
    "        # https://stackoverflow.com/questions/925024/how-can-i-remove-the-top-and-right-axis-in-matplotlib\n",
    "        ax_item[0].spines['right'].set_visible(False)\n",
    "        ax_item[0].spines['top'].set_visible(False)\n",
    "\n",
    "        # To make the legend for this mo fo is going to be a little tricky. The legend argument basically takes\n",
    "        # two lists. The first list should contain a rectangle object for each of the sequences (this\n",
    "        # will be the coloured box). We get this object from the bar objects that we are creating.\n",
    "        # The second list is a list of the labels. This should be easier. We can just use the\n",
    "        # most_abund_seq_names[:30] for these.\n",
    "        # to grab the rectangles, I think its best to pick them up during the plotting and hold them in a list\n",
    "        # outside of the subplot loop. We will need a holder for the objects to populate.\n",
    "\n",
    "        #https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_xticks.html#matplotlib.axes.Axes.set_xticks\n",
    "        ax_item[0].set_yticks([1], minor=False)\n",
    "        #https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html#matplotlib.axes.Axes.set_xticklabels\n",
    "        ax_item[0].set_yticklabels(['1'])\n",
    "        # if ax_item[2] == '22.08.2016':\n",
    "        ax_item[0].set_ylabel(ax_item[1])\n",
    "\n",
    "        # # only if we're on the final plot then we should add the date ticks\n",
    "        # if ax_item[1] == 'turf':\n",
    "        #     ax_item[0].set_xlabel(ax_item[2])\n",
    "        # # also if we are on the mucus we should put the labels for the dates on the axis\n",
    "        # if ax_item[1] == 'mucus':\n",
    "        #     ax_item[0].set_xlabel(ax_item[2])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #https://stackoverflow.com/questions/16150819/common-xlabel-ylabel-for-matplotlib-subplots/26892326\n",
    "    plt.text(0, 0.5, 'ITS2 sequence relative abundance', va='center', rotation='vertical')\n",
    "    ordered_rectange_list = [sub_list[0] for sub_list in legend_rectangle_holder]\n",
    "    plt.legend(ordered_rectange_list, top_n_seqs_to_get, loc='lower center')\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig('env_smp_diversity_bars_all_dates_grouped.svg')\n",
    "    plt.savefig('env_smp_diversity_bars_all_dates_grouped.png')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversity bars all dates grouped and sed grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diverstiy_figs_all_dates_grouped_dates_sed_grouped():\n",
    "    '''\n",
    "    This is yet another development in which we will group all of the dates together rather than sorting them\n",
    "    individually.\n",
    "    This is a development from the above code. Here in this code we will plot all of the date replicates\n",
    "    for the env_samples as these could in theory aid a little bit in the diversity comparison.\n",
    "    I will put these on the same axis as the samples of the same type with a gap or so inbetween.\n",
    "    This will be the code to create the diversity figure for Gabi's paper.\n",
    "        We have used symportal to do the QC work on the samples.\n",
    "        As such we have tab delimited outputs that we can read into pandas dataframes\n",
    "        We also have the info list from Gabi that contains what all of the samples are\n",
    "        Esentially what we want to do is quite simple, however, we will need to be careful with the colouring\n",
    "        As there are going to be a shed ton of sequences we will probably want to order the sequences found some how\n",
    "        and colour those, then maybe colour sequences found at below 5% to grey. Something like that.\n",
    "        I have a load of code from previous work when we were trying to look for actual SymPortal ITS2 type profiles\n",
    "        within the env samples but we are no longer doing that. Still some of the code below might be useful.\n",
    "        Actually I'm going to drop it into a junk code def and start from scratch here.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        sp_output_df = pickle.load(open('sp_output_df_all_dates.pickle', 'rb'))\n",
    "        QC_info_df= pickle.load(open('QC_info_df_all_dates.pickle', 'rb'))\n",
    "        info_df = pickle.load(open('info_df_all_dates.pickle', 'rb'))\n",
    "    except:\n",
    "        # read in the SymPortal output\n",
    "        sp_output_df = pd.read_csv('131_142.DIVs.absolute.txt', sep='\\t', lineterminator='\\n')\n",
    "\n",
    "        # The SP output contains the QC info columns between the DIVs and the no_name ITS2 columns.\n",
    "        # lets put the QC info columns into a seperate df.\n",
    "        QC_info_df = sp_output_df[['Samples','raw_contigs', 'post_qc_absolute_seqs', 'post_qc_unique_seqs',\n",
    "                                'post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs',\n",
    "                                'post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs',\n",
    "                                   'size_screening_violation_absolute', 'size_screening_violation_unique',\n",
    "                                   'post_med_absolute', 'post_med_unique']]\n",
    "\n",
    "        # now lets drop the QC columns from the SP output df and also drop the clade summation columns\n",
    "        # we will be left with just clumns for each one of the sequences found in the samples\n",
    "        sp_output_df.drop(columns=['noName Clade A', 'noName Clade B', 'noName Clade C', 'noName Clade D',\n",
    "                                'noName Clade E', 'noName Clade F', 'noName Clade G', 'noName Clade H',\n",
    "                                'noName Clade I', 'raw_contigs', 'post_qc_absolute_seqs', 'post_qc_unique_seqs',\n",
    "                                'post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs',\n",
    "                                'post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs',\n",
    "                                   'size_screening_violation_absolute', 'size_screening_violation_unique',\n",
    "                                   'post_med_absolute', 'post_med_unique'\n",
    "                                   ]\n",
    "                          , inplace=True)\n",
    "\n",
    "        # read in the info file\n",
    "        info_df = pd.read_csv('info_300718.csv')\n",
    "\n",
    "        # we no longer drop the dates\n",
    "        # drop the rows that aren't from the 22.08.2016 data\n",
    "        info_df = info_df[info_df['coral genus'] != 'Sponge']\n",
    "        # need to use the ampersand rather than 'and': https://stackoverflow.com/questions/36921951/truth-value-of-a-series-is-ambiguous-use-a-empty-a-bool-a-item-a-any-o\n",
    "        info_df = info_df[(info_df['Sample no.'] != 'negative extration') & (info_df['Sample no.'] != 'negative pcr') & (info_df['Sample no.'] != 'DIV_accession')]\n",
    "\n",
    "        # Now we need to link the SP output to the sample names in the excel. Annoyingly they are formatted\n",
    "        # slightly differently so we can't make a direct comparison.\n",
    "        # easiest way to link them is to see if the first part of the SP name is the same as the first part\n",
    "        # of the 'sequence file' in the meta info\n",
    "        # when doing this we can also drop the SP info for those samples that won't be used i.e. those that\n",
    "        # aren't now in the info_df\n",
    "\n",
    "        # firstly rename the colums so that they are 'sample_name' in all of the dfs\n",
    "        QC_info_df.rename(index=str, columns={'Samples': 'sample_name'}, inplace=True)\n",
    "        sp_output_df.rename(index=str, columns={'Samples': 'sample_name'}, inplace=True)\n",
    "        info_df.rename(index=str, columns={'Sample Name': 'sample_name'}, inplace=True)\n",
    "\n",
    "        indices_to_drop = []\n",
    "        for sp_index in sp_output_df.index.values.tolist():\n",
    "            # keep track of whether the sp_index was found in the info table\n",
    "            # if it wasn't then it should be dropped\n",
    "            sys.stdout.write('\\rsp_index: {}'.format(sp_index))\n",
    "            found = False\n",
    "            for info_index in info_df.index.values.tolist():\n",
    "                if sp_output_df.loc[sp_index, 'sample_name'].split('_')[0] == info_df.loc[info_index, 'Sequence file'].split('_')[0]:\n",
    "                    found = True\n",
    "                    # then these are a related set of rows and we should make the sample_names the same\n",
    "                    sp_output_df.loc[sp_index, 'sample_name'] = info_df.loc[info_index, 'sample_name']\n",
    "                    QC_info_df.loc[sp_index, 'sample_name'] = info_df.loc[info_index, 'sample_name']\n",
    "\n",
    "\n",
    "            if not found:\n",
    "                indices_to_drop.append(sp_index)\n",
    "\n",
    "        # drop the rows from the SP output tables that aren't going to be used\n",
    "        sp_output_df.drop(inplace=True, index=indices_to_drop)\n",
    "        QC_info_df.drop(inplace=True, index=indices_to_drop)\n",
    "\n",
    "        # let's sort out the 'environ type' column in the info_df\n",
    "        # currently it is a bit of a mess\n",
    "        for index in info_df.index.values.tolist():\n",
    "            if 'coral' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'coral'\n",
    "            elif 'seawater' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'sea_water'\n",
    "            elif 'mucus' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'mucus'\n",
    "            elif 'SA' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'sed_close'\n",
    "            elif 'SB' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'sed_far'\n",
    "            elif 'TA' in info_df.loc[index, 'Sample no.']:\n",
    "                info_df.loc[index, 'environ type'] = 'turf'\n",
    "\n",
    "        # now clean up the df indices\n",
    "        info_df.index = range(len(info_df))\n",
    "        sp_output_df.index = range(len(sp_output_df))\n",
    "        QC_info_df.index = range(len(QC_info_df))\n",
    "\n",
    "        # make the sample_name column the index for each of the datasets\n",
    "        info_df.set_index('sample_name', inplace=True)\n",
    "        sp_output_df.set_index('sample_name', inplace=True)\n",
    "        QC_info_df.set_index('sample_name', inplace=True)\n",
    "\n",
    "        # pickle the out put and put a check in place to see if we need to do the above\n",
    "        pickle.dump(sp_output_df, open('sp_output_df_all_dates.pickle', 'wb'))\n",
    "        pickle.dump(QC_info_df, open('QC_info_df_all_dates.pickle', 'wb'))\n",
    "        pickle.dump(info_df, open('info_df_all_dates.pickle', 'wb'))\n",
    "\n",
    "\n",
    "    info_df = info_df.applymap(lambda x: 'sed' if \"sed\" in str(x) else x)\n",
    "\n",
    "    # so we want to plot the ITS2 sequence diversity in each of the samples as bar charts\n",
    "    # We are going to have a huge diversity of sequences to deal with so I think something along the lines\n",
    "    # of plotting the top n most abundant sequences. The term 'most abundant' should be considered carefully here\n",
    "    # I think it will be best if we work on a sample by sample basis. i.e. we pick the n sequences that have the\n",
    "    # highest representation in any one sample. So for example what we are not doing is seeing how many times\n",
    "    # C3 was sequenced across all of the samples, and finding that it is a lot and therefore plotting it.\n",
    "    # we are looking in each of the samples and seeing the highest proportion it is found at in any one sample.\n",
    "    # This way we should have the best chance of having a coloured representation for each sample's most abundant\n",
    "    # sequence.\n",
    "\n",
    "    # to start lets go sample by sample and see what the highest prop for each seq is.\n",
    "\n",
    "    # dict to hold info on which sample and what the proportion is for each sequence\n",
    "    # key = sequence name, value = tup ( sample name, relative abundance)\n",
    "    try:\n",
    "        seq_rel_abund_calculator_dict = pickle.load(open('seq_rel_abund_calculator_dict_all_dates.pickle', 'rb'))\n",
    "    except:\n",
    "        seq_rel_abund_calculator_dict = {}\n",
    "        for sample_index in sp_output_df.index.values.tolist():\n",
    "            sys.stdout.write('\\nGeting rel seq abundances from {}\\n'.format(sample_index))\n",
    "            # temp_prop_array = sp_output_df.loc[sample_index].div(sp_output_df.loc[sample_index].sum(axis='index'))\n",
    "            temp_prop_array = sp_output_df.loc[sample_index].div(sp_output_df.loc[sample_index].sum())\n",
    "            for seq_name in temp_prop_array.keys():\n",
    "                sys.stdout.write('\\rseq: {}'.format(seq_name))\n",
    "                val = temp_prop_array[seq_name]\n",
    "                if val != 0:  # if the sequences was found in the sample\n",
    "                    # If the sequence is already in the dict\n",
    "                    if seq_name in seq_rel_abund_calculator_dict.keys():\n",
    "                        # check to seee if the rel abundance is larger than the one already logged\n",
    "                        if val > seq_rel_abund_calculator_dict[seq_name][1]:\n",
    "                            seq_rel_abund_calculator_dict[seq_name] = (sample_index, val)\n",
    "                    # if we haven't logged for this sequence yet, then add this as the first log\n",
    "                    else:\n",
    "                        seq_rel_abund_calculator_dict[seq_name] = (sample_index, val)\n",
    "        pickle.dump(seq_rel_abund_calculator_dict, open('seq_rel_abund_calculator_dict_all_dates.pickle', 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "    # here we have a dict that contains the largest rel_abundances per sample for each of the seqs\n",
    "    # now we can sort this to look at the top ? n sequences to start with (I'm not sure how the colouring will\n",
    "    # look like so lets just start with 30 and see where we get to)\n",
    "    sorted_list = sorted(seq_rel_abund_calculator_dict.items(), key = lambda x: x[1][1], reverse=True)\n",
    "    most_abund_seq_names = [tup[0] for tup in sorted_list]\n",
    "\n",
    "    # from the above sorted list we can then plot these sequences with colour and all others with grey scale\n",
    "    # lets make a coulour dictionary for the most common types\n",
    "    colour_list = get_colour_list()\n",
    "    colour_dict = {}\n",
    "    num_coloured_seqs = 30\n",
    "\n",
    "    # we will also need a grey palette for those sequences that are not the ones being annotated\n",
    "    grey_palette = ['#D0CFD4', '#89888D', '#4A4A4C', '#8A8C82', '#D4D5D0', '#53544F']\n",
    "\n",
    "    # make the dict\n",
    "    for i in range(len(most_abund_seq_names)):\n",
    "        if i < num_coloured_seqs:\n",
    "            colour_dict[most_abund_seq_names[i]] = colour_list[i]\n",
    "        else:\n",
    "            colour_dict[most_abund_seq_names[i]] = grey_palette[i % 6]\n",
    "\n",
    "    # add the 'low' key and assign it to grey for later on\n",
    "    # the low category will be created later on to hold the grouped abundances of sequences in samples\n",
    "    # below a certain rel abund cutoff\n",
    "    colour_dict['low'] = '#D0CFD4'\n",
    "\n",
    "    # for plotting we will also need to collect the 'rectangle' objects from the plotting process to use to make\n",
    "    # the legend which we will display on the plot right at the end\n",
    "    # this is going to be a little tricky to collect as not every sample type group that we are plotting\n",
    "    # is going to have all of the top n sequences. So, we will have to pick up rectangles when they come up in\n",
    "    # the plotting.\n",
    "    # to keep track of which sequences we need we have:\n",
    "    top_n_seqs_to_get = most_abund_seq_names[:num_coloured_seqs]\n",
    "    # to keep track of which sequences we have already collected we have:\n",
    "    top_n_seqs_done = []\n",
    "    #finally to collect the rectangles and store them in order despite collecting them out of order we have:\n",
    "    legend_rectangle_holder = [[] for i in range(num_coloured_seqs)]\n",
    "\n",
    "\n",
    "    # set up the plotting environment\n",
    "\n",
    "\n",
    "    # rather than work with the subplot standard layout we will use subplot2grid for our set up and have some hard\n",
    "    # coded axes\n",
    "\n",
    "    this_fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    ax_list = [\n",
    "        # coral\n",
    "        (plt.subplot2grid((6, 67), (0, 0), colspan=67), 'coral'),\n",
    "\n",
    "        # mucus\n",
    "        (plt.subplot2grid((6, 67), (1, 0), colspan=66), 'mucus'),\n",
    "\n",
    "        # sea water\n",
    "        (plt.subplot2grid((6, 67), (2, 0 ),  colspan=20), 'sea_water'),\n",
    "\n",
    "        # sed_close\n",
    "        (plt.subplot2grid((6, 67), (3, 0 ), colspan=40), 'sed'),\n",
    "\n",
    "        # turf,\n",
    "        (plt.subplot2grid((6, 67), (4, 0 ), colspan= 37), 'turf'),\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    # now we need to get the actual plotting information for each of the samples.\n",
    "    # we can do this sample type by sample type\n",
    "    # we will create a local dataframe that will be a sub set of the main sp output dataframe but will\n",
    "    # only contain the samples of the given sample type. It will also eventually only contain the sequence\n",
    "    # information for the sample type in question. Normally I would make a 2D list to hold the plotting\n",
    "    # information but in this case having all of the information in a dataframe is working really well and\n",
    "    # this is how I will do it in future.\n",
    "\n",
    "\n",
    "    for ax_item in ax_list:\n",
    "        try:\n",
    "            env_sp_output_df_prop = pickle.load(open('env_sp_output_df_prop_{}_grouped_sed_grouped.pickle'.format(ax_item[1]), 'rb'))\n",
    "            sorted_list_of_env_specific_seqs = pickle.load(open('sorted_list_of_env_specific_seqs_{}_grouped_sed_grouped.pickle'.format(ax_item[1]), 'rb'))\n",
    "        except:\n",
    "            sys.stdout.write('\\nGenerating plotting info for samples {}\\n'.format(ax_item[1]))\n",
    "            # currently we have something like 4000 sequences to plot which is too may\n",
    "            # I think it will be much easier if we group the sequences that are found below a certain\n",
    "            # threshold. I think the best way to do this is to create a slice of the main df that\n",
    "            # contain the information for the samples of the env_type only\n",
    "\n",
    "            # get subset of the main dfs that contain only the env_type samples from the given date\n",
    "            env_info_df = info_df[(info_df['environ type'] == ax_item[1])]\n",
    "\n",
    "            # we need to drop any samples that have only zeros in their columns\n",
    "            # https://stackoverflow.com/questions/22649693/drop-rows-with-all-zeros-in-pandas-data-frame\n",
    "            env_info_df = env_info_df[(env_info_df.T != 0).any()]\n",
    "\n",
    "\n",
    "            env_sp_output_df = sp_output_df.loc[env_info_df.index.values.tolist()]\n",
    "\n",
    "            # we need to drop any samples that have only zeros in their columns\n",
    "            # https://stackoverflow.com/questions/22649693/drop-rows-with-all-zeros-in-pandas-data-frame\n",
    "            env_sp_output_df = env_sp_output_df[(env_sp_output_df.T != 0).any()]\n",
    "\n",
    "            # append a 'low' columns to the env_sp_ouput_df and populate with 0s\n",
    "            env_sp_output_df['low'] = 0\n",
    "            # now make a proportions version of the df, rather than absolute counts\n",
    "            env_sp_output_df_prop = env_sp_output_df[:].div(env_sp_output_df[:].sum(axis=1), axis=0)\n",
    "\n",
    "            # now as we work our way through we can sum up the low sequences into this column\n",
    "            # we can then check for 0 columns and drop these.\n",
    "\n",
    "            # get a list of the sequences found in the collection of samples of the given type and order\n",
    "            # them according to summed rel_abundance acorss all samples. This should be the order in which\n",
    "            # the samples are plotted\n",
    "            # at the same time we can get the info we need for plotting\n",
    "\n",
    "            summed_seq_rel_abund_across_smpls_dict = {seq: 0 for seq in list(sp_output_df)}\n",
    "\n",
    "            # we are also going to need to put the samples in an order that makes sense.\n",
    "            # Ideally I would like to take the time to do a proper travelling salesman analysis\n",
    "            # and I was thinking of putting in an ant colony system to do this but...\n",
    "            # I'm not sure we've got time for that. So let's do something a little more simple which\n",
    "            # should still be effective and look groovy\n",
    "            # Lets sort according to the majority sequence. I.e. lets put the samples into groups that\n",
    "            # are defined by what their majorty sequence is, then lets plot in order of those groups.\n",
    "            # within the groups we will plot in the order of the most abundant rel abund first.\n",
    "            # to get this information we will have a dict to collect it. The key of the dict will\n",
    "            # be the majority sequence with the value being a list that contains tuples. One tuple\n",
    "            # for each sample in the list which will contain the sample_name and the re abund of the maj seq\n",
    "\n",
    "            # Dict for holding the sample_sorting info\n",
    "            sample_sorting_info_dict = defaultdict(list)\n",
    "\n",
    "            for sp_index in env_sp_output_df_prop.index.values.tolist():\n",
    "                sys.stdout.write('\\r{}'.format(sp_index))\n",
    "                # we need to get the name of the most abundant sequence and its rel abund for each sample\n",
    "                # for the sample_sorting_info_dict\n",
    "                most_abund_seq_name = env_sp_output_df_prop.loc[sp_index].idxmax(axis='index')\n",
    "\n",
    "                rel_abund_of_most_abund_seq = env_sp_output_df_prop.loc[sp_index, most_abund_seq_name]\n",
    "\n",
    "                apples = 'asdf'\n",
    "                sample_sorting_info_dict[most_abund_seq_name].append((sp_index, rel_abund_of_most_abund_seq))\n",
    "                # Put its seq rel abundances ino the summed_seq_rel_abund_across... dict\n",
    "\n",
    "                for non_zero_seq in env_sp_output_df_prop.loc[sp_index][env_sp_output_df_prop.loc[sp_index] > 0].index:  # not including the final 'low' columns (this will be zero)\n",
    "                    val = env_sp_output_df_prop.loc[sp_index, non_zero_seq]\n",
    "                    # be sure to count the value of this cell and using it in judging which are the most\n",
    "                    # abundant sequences before we check whether to relegate it to the 'low' column\n",
    "                    summed_seq_rel_abund_across_smpls_dict[non_zero_seq] += val\n",
    "                    if val < 0.005:\n",
    "                        env_sp_output_df_prop.loc[sp_index, 'low'] += val\n",
    "                        env_sp_output_df_prop.loc[sp_index, non_zero_seq] = 0\n",
    "\n",
    "\n",
    "            # here we can get a sorted sample list using the sample_sorting_info_dict\n",
    "            sorted_sample_list = []\n",
    "            # we want to work through the sample_sorting_info_dict by the longest lists first\n",
    "            sorted_keys_for_sample_sort = \\\n",
    "                [tup[0] for tup in sorted(sample_sorting_info_dict.items(), key=lambda x: len(x[1]), reverse=True)]\n",
    "            # for each of teh maj seq groups\n",
    "            for sorted_key in sorted_keys_for_sample_sort:\n",
    "                # now within each of these lists we want to order according to the rel_abundance of the sequences\n",
    "                sorted_list_of_samples_of_group = [tup[0] for tup in sorted(sample_sorting_info_dict[sorted_key], key=lambda x: x[1], reverse=True)]\n",
    "                sorted_sample_list.extend(sorted_list_of_samples_of_group)\n",
    "\n",
    "            # now we should re-order the df so that it is in the sample order of sorted_sample_list\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop.reindex(sorted_sample_list)\n",
    "\n",
    "            # here we have a dict that contains the abundances of the seqs for the coral samples\n",
    "            # we will plot the coral samples' seqs in the order of the sequences in this dict\n",
    "            sorted_list_of_env_specific_seqs_tup \\\n",
    "                = sorted(summed_seq_rel_abund_across_smpls_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "            sorted_list_of_env_specific_seqs = [tup[0] for tup in sorted_list_of_env_specific_seqs_tup]\n",
    "\n",
    "            # Now we check for zero only columns and drop them\n",
    "            # we also need to remove these columns from the sorted\n",
    "            non_zero_cols = list(env_sp_output_df_prop.loc[:, (env_sp_output_df_prop != 0).any(axis=0)])\n",
    "            sorted_list_of_env_specific_seqs = [seq for seq in sorted_list_of_env_specific_seqs if seq in non_zero_cols]\n",
    "\n",
    "            # add the 'low' column only if the 'low' column is a non-zero column, i.e. used\n",
    "            if 'low' in non_zero_cols:\n",
    "                sorted_list_of_env_specific_seqs.append('low')\n",
    "\n",
    "            # now drop the cols\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop[non_zero_cols]\n",
    "\n",
    "            # we also have the plotting list which contains the info we will be plotting.\n",
    "\n",
    "            # the plotting_list is currently organised in a different order to that of the sorted_list_of_env...\n",
    "            # we need to change this order\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop[sorted_list_of_env_specific_seqs]\n",
    "\n",
    "            # we now need to transpose this\n",
    "            env_sp_output_df_prop = env_sp_output_df_prop.transpose()\n",
    "\n",
    "            # pickle the info we've been plotting\n",
    "            pickle.dump(env_sp_output_df_prop, open('env_sp_output_df_prop_{}_grouped_sed_grouped.pickle'.format(ax_item[1]), 'wb'))\n",
    "            pickle.dump(sorted_list_of_env_specific_seqs, open('sorted_list_of_env_specific_seqs_{}_grouped_sed_grouped.pickle'.format(ax_item[1]), 'wb'))\n",
    "\n",
    "        # here we finally have the plotting lists in the order of the sorted_list_of_env\n",
    "        bottom = [0 for smp in list(env_sp_output_df_prop)]\n",
    "        bar_list = []\n",
    "        # for each sample\n",
    "        plotting_indices = range(len(list(env_sp_output_df_prop)))\n",
    "\n",
    "        # for each sequence\n",
    "        list_of_seqs = env_sp_output_df_prop.index.values.tolist()\n",
    "        for i in range(len(list_of_seqs)):\n",
    "\n",
    "            sys.stdout.write('\\r{}/{}'.format(i, len(list(env_sp_output_df_prop.iloc[:, 0]))))\n",
    "            bar_list.append(ax_item[0].bar(plotting_indices, list(env_sp_output_df_prop.iloc[i]), 1, bottom,\n",
    "                                                   color=colour_dict[sorted_list_of_env_specific_seqs[i]]))\n",
    "            bottom = [L + M for L, M in zip(bottom, list(env_sp_output_df_prop.iloc[i]))]\n",
    "\n",
    "            # check to see if the seq we are plotting is still in the top_n_seqs list\n",
    "            # if it is in this list then we still need to grab a rectangle for plotting\n",
    "            # the legend. Once we have grabbed the rectangle then we should remove the seq from the top_n_seqs list\n",
    "            seq_name = list_of_seqs[i]\n",
    "\n",
    "            if seq_name in top_n_seqs_to_get and seq_name not in top_n_seqs_done:\n",
    "                # then this is a seq that we still need to get a rectangle for the legend\n",
    "                legend_rectangle_holder[top_n_seqs_to_get.index(seq_name)].append(bar_list[-1][0])\n",
    "                top_n_seqs_done.append(seq_name)\n",
    "\n",
    "        # https://stackoverflow.com/questions/12998430/remove-xticks-in-a-matplotlib-plot\n",
    "        ax_item[0].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "        # https://stackoverflow.com/questions/15858192/how-to-set-xlim-and-ylim-for-a-subplot-in-matplotlib\n",
    "        ax_item[0].set_xlim((-0.5, (len(list(env_sp_output_df_prop)) - 0.5)))\n",
    "        # https://stackoverflow.com/questions/925024/how-can-i-remove-the-top-and-right-axis-in-matplotlib\n",
    "        ax_item[0].spines['right'].set_visible(False)\n",
    "        ax_item[0].spines['top'].set_visible(False)\n",
    "\n",
    "        # To make the legend for this mo fo is going to be a little tricky. The legend argument basically takes\n",
    "        # two lists. The first list should contain a rectangle object for each of the sequences (this\n",
    "        # will be the coloured box). We get this object from the bar objects that we are creating.\n",
    "        # The second list is a list of the labels. This should be easier. We can just use the\n",
    "        # most_abund_seq_names[:30] for these.\n",
    "        # to grab the rectangles, I think its best to pick them up during the plotting and hold them in a list\n",
    "        # outside of the subplot loop. We will need a holder for the objects to populate.\n",
    "\n",
    "        #https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_xticks.html#matplotlib.axes.Axes.set_xticks\n",
    "        ax_item[0].set_yticks([1], minor=False)\n",
    "        #https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.set_xticklabels.html#matplotlib.axes.Axes.set_xticklabels\n",
    "        ax_item[0].set_yticklabels(['1'])\n",
    "        # if ax_item[2] == '22.08.2016':\n",
    "        ax_item[0].set_ylabel(ax_item[1])\n",
    "\n",
    "\n",
    "    #https://stackoverflow.com/questions/16150819/common-xlabel-ylabel-for-matplotlib-subplots/26892326\n",
    "    plt.text(0, 0.5, 'ITS2 sequence relative abundance', va='center', rotation='vertical')\n",
    "    ordered_rectange_list = [sub_list[0] for sub_list in legend_rectangle_holder]\n",
    "    plt.legend(ordered_rectange_list, top_n_seqs_to_get, loc='lower center')\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig('env_smp_diversity_bars_all_dates_grouped_sed_grouped.svg')\n",
    "    plt.savefig('env_smp_diversity_bars_all_dates_grouped_sed_grouped.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversity stats one date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quan_diversity_figs():\n",
    "    ''' The purpose of these figs will be to compare the diversity found in the different sample types\n",
    "    I envisage them being a sub plot each step of the QC:\n",
    "     1 - Post QC\n",
    "     2 - Symbiodinium\n",
    "     3 - non-Symbiodinium\n",
    "     4 - post - MED\n",
    "     5 - pst MED to pre MED symbiodinium ratio\n",
    "     number 5 will be important to consider and will hopefully hopefully be similar between all of the sample\n",
    "     types. It is important because MED has the possibility to collapse diverstiy in a non-linear manner.\n",
    "     e.g. in a sample with 1000 sequences, that has roughly the same spread as a sample with only 100 sequences\n",
    "     it is possible that MED will collapse both of these samples into 10 meaningful sequences. However, we would\n",
    "     then need to make sure to compare the different samples according to their pre-MED diversity. Although, we can\n",
    "     also discuss when the post-MED diversity means.\n",
    "     For each of teh sup plots I envisage there being a a 'set of plots' for each of the sample types\n",
    "     For each of these sets would in trun contain a set of two plots. One for absolute and one for unique. Each one\n",
    "     of these plots would be the actual datapoints on the left, and then a mean point with SD bars to the right of it\n",
    "     . We can put the absoulte and unique values on different axes as the differences between these will be huge.\n",
    "     We can achieve this by setting up subplot axes and then using the .errorbar and .scatter functions.\n",
    "     In terms of collecting the data we should simply use the cleaned up dataframes that we created when making the\n",
    "     diversity plots.'''\n",
    "\n",
    "    sp_output_df = pickle.load(open('sp_output_df.pickle', 'rb'))\n",
    "    QC_info_df = pickle.load(open('QC_info_df.pickle', 'rb'))\n",
    "    info_df = pickle.load(open('info_df.pickle', 'rb'))\n",
    "\n",
    "    # lets make 5 subplot\n",
    "    # according to the above categories\n",
    "\n",
    "    f, axarr = plt.subplots(5, 1)\n",
    "    # counter to reference which set of axes we are plotting on\n",
    "    axarr_index = 0\n",
    "    # y_axis_labels = ['raw_contigs', 'post_qc', 'Symbiodinium', 'non-Symbiodinium', 'post-MED', 'post-MED / pre-MED']\n",
    "    y_axis_labels = ['raw_contigs',  'non-Symbiodinium','Symbiodinium',  'post-MED', 'post-MED / pre-MED']\n",
    "\n",
    "\n",
    "    # cycle through these strings to help us with our conditionals\n",
    "    # one of these for each of the subplots that we will create\n",
    "    # we will make these useful tuples that will hold the actual name of the columns that the data we want will\n",
    "    # be in so that we can pull these out of the dataframe easily\n",
    "    # for sub_plot_type in [('raw_contigs',), ('post_qc_absolute_seqs', 'post_qc_unique_seqs'),\n",
    "    #                       ('post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs'),\n",
    "    #                       ('post_taxa_id_absolute_non_symbiodinium_seqs','post_taxa_id_unique_non_symbiodinium_seqs'),\n",
    "    #                       ('post_med_absolute','post_med_unique'),\n",
    "    #                       ('med_ratio', True) ]:\n",
    "    for sub_plot_type in [('raw_contigs',),\n",
    "                          ('post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs'),\n",
    "                          ('post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs'),\n",
    "                          ('post_med_absolute', 'post_med_unique'),\n",
    "                          ('med_ratio', True)]:\n",
    "\n",
    "\n",
    "        # for each of the sub plots we will want to grab the absolute and unique counts and plot these\n",
    "        # for each of the sample types.\n",
    "        # go environment type by environment type\n",
    "\n",
    "        # we will create some x axis indicies to arranage where we will be ploting\n",
    "        # we can be smart with these later on and create some nice spacing layouts but for the time\n",
    "        # being lets just get things plotted. Let's have one idices for each sample type and work\n",
    "        # relatively from there.\n",
    "        ind = range(6)\n",
    "        ind_index = 0\n",
    "\n",
    "\n",
    "        if sub_plot_type[0] != 'raw_contigs': ax2 = axarr[axarr_index].twinx()\n",
    "\n",
    "        axarr[axarr_index].set_xlabel(y_axis_labels[axarr_index])\n",
    "        env_types_list = ['coral', 'mucus', 'sea_water', 'sed_close', 'sed_far', 'turf']\n",
    "        for env_type in env_types_list:\n",
    "\n",
    "\n",
    "            if sub_plot_type[0] == 'raw_contigs':\n",
    "                # here we will plot just the raw_contigs\n",
    "                # get a sub df of the main df according to the env_type\n",
    "                # get subset of the main dfs that contain only the coral samples\n",
    "                env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "                env_QC_info_df = QC_info_df.loc[env_info_df.index.values.tolist()]\n",
    "                sys.stdout.write('\\nGenerating plotting info for {} samples in subplot type {}\\n'\n",
    "                                 .format(env_type, sub_plot_type))\n",
    "                # the data we are going to be plotting is so simple that rather than collecting it and then\n",
    "                # plotting it we may as well just go straight to plotting it from the df\n",
    "\n",
    "                # PLOT ABSOLUTE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "                y_values = list(env_QC_info_df.loc[:, sub_plot_type[0]])\n",
    "                x_values = [ind[ind_index] for y in y_values]\n",
    "                axarr[axarr_index].scatter(x_values, y_values, marker='.', s=1, c='b')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                # I know there is a mean and SD function on a pandas series but it is throwing out all sorts of\n",
    "                # erros so lest stick with what we know\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "                axarr[axarr_index].scatter(x=ind[ind_index] + 0.125, y=mean, marker='s', s=8, c='b')\n",
    "                axarr[axarr_index].errorbar(x=ind[ind_index] + 0.125, y=mean, yerr=std, fmt='none', c='b')\n",
    "\n",
    "                if env_type == 'turf':\n",
    "                    axarr[axarr_index].set_ylabel('', color='b')\n",
    "                    axarr[axarr_index].tick_params('y', colors='b')\n",
    "                    axarr[axarr_index].spines['right'].set_visible(False)\n",
    "                    # axarr[axarr_index].spines['bottom'].set_visible(False)\n",
    "                    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "\n",
    "                    # set the ticks\n",
    "                    # axarr[axarr_index].set_xticks([a + 0.1875 for a in range(6)], minor=False)\n",
    "                    # axarr[axarr_index].set_xticklabels(env_types_list)\n",
    "                    axarr[axarr_index].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "                    # set the xaxis title\n",
    "                    axarr[axarr_index].set_xlabel('raw_contigs')\n",
    "\n",
    "                    axarr[axarr_index].set_ylim((0, 1200000))\n",
    "\n",
    "                ind_index += 1\n",
    "            elif sub_plot_type[0] != 'med_ratio':\n",
    "                # get a sub df of the main df according to the env_type\n",
    "                # get subset of the main dfs that contain only the coral samples\n",
    "                env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "                env_QC_info_df = QC_info_df.loc[env_info_df.index.values.tolist()]\n",
    "                sys.stdout.write('\\nGenerating plotting info for {} samples in subplot type {}\\n'\n",
    "                                 .format(env_type, sub_plot_type))\n",
    "                # the data we are going to be plotting is so simple that rather than collecting it and then\n",
    "                # plotting it we may as well just go straight to plotting it from the df\n",
    "\n",
    "                # PLOT ABSOLUTE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "                y_values = list(env_QC_info_df.loc[:, sub_plot_type[0]])\n",
    "                x_values = [ind[ind_index] for y in y_values]\n",
    "                axarr[axarr_index].scatter(x_values, y_values, marker='.', s=1, c='b')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                # I know there is a mean and SD function on a pandas series but it is throwing out all sorts of\n",
    "                # erros so lest stick with what we know\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "                axarr[axarr_index].scatter(x=ind[ind_index] + 0.125, y=mean, marker='s', s=8, c='b')\n",
    "                axarr[axarr_index].errorbar(x=ind[ind_index] + 0.125, y=mean, yerr=std, fmt='none', c='b')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    axarr[axarr_index].set_ylabel('', color='b')\n",
    "                    axarr[axarr_index].tick_params('y', colors='b')\n",
    "\n",
    "\n",
    "                # PLOT UNIQUE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "\n",
    "                y_values = list(env_QC_info_df.loc[:, sub_plot_type[1]])\n",
    "                x_values = [ind[ind_index] + 0.250 for y in y_values]\n",
    "                ax2.scatter(x_values, y_values, marker='.', s=1, c='r')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "\n",
    "                ax2.scatter(x=ind[ind_index] + 0.375, y=mean, marker='o', s=8, c='r')\n",
    "                ax2.errorbar(x=ind[ind_index] + 0.375, y=mean, yerr=std, fmt='none', c='r')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    ax2.set_ylabel( '', color='r')\n",
    "                    ax2.tick_params('y', colors='r')\n",
    "                    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "                    # axarr[axarr_index].spines['bottom'].set_visible(False)\n",
    "                    ax2.spines['top'].set_visible(False)\n",
    "                    # ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "                    # axarr[axarr_index].set_xticks([a + 0.1875 for a in range(6)], minor=False)\n",
    "                    # axarr[axarr_index].set_xticklabels(env_types_list)\n",
    "                    axarr[axarr_index].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "                    ax2.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "                    axarr[axarr_index].set_xlabel(y_axis_labels[axarr_index])\n",
    "                    if sub_plot_type[0] == 'post_taxa_id_absolute_non_symbiodinium_seqs':\n",
    "                        axarr[axarr_index].set_ylim((0, 150000))\n",
    "                    elif sub_plot_type[0] == 'post_taxa_id_absolute_symbiodinium_seqs':\n",
    "                        axarr[axarr_index].set_ylim((0, 800000))\n",
    "                    elif sub_plot_type[0] == 'post_med_absolute':\n",
    "                        axarr[axarr_index].set_ylim((0, 800000))\n",
    "\n",
    "                ind_index += 1\n",
    "            else:\n",
    "                # here we need to ploto out the MED ratios.\n",
    "                # these are simply going to be the med abosultes divided by the symbiodinium absolutes\n",
    "                # and same for the uniques.\n",
    "                # get a sub df of the main df according to the env_type\n",
    "                # get subset of the main dfs that contain only the coral samples\n",
    "\n",
    "                env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "                env_QC_info_df = QC_info_df.loc[env_info_df.index.values.tolist()]\n",
    "                sys.stdout.write('\\nGenerating plotting info for {} samples in subplot type {}\\n'\n",
    "                                 .format(env_type, sub_plot_type))\n",
    "                # the data we are going to be plotting is so simple that rather than collecting it and then\n",
    "                # plotting it we may as well just go straight to plotting it from the df\n",
    "\n",
    "                # PLOT ABSOLUTE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "                y_values = [tup[0] / tup[1] for tup in zip(list(env_QC_info_df.loc[:, 'post_med_absolute']), list(\n",
    "                    env_QC_info_df.loc[:, 'post_taxa_id_absolute_symbiodinium_seqs']))]\n",
    "                x_values = [ind[ind_index] for y in y_values]\n",
    "                axarr[axarr_index].scatter(x_values, y_values, marker='.', s=1, c='b')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                # I know there is a mean and SD function on a pandas series but it is throwing out all sorts of\n",
    "                # erros so lest stick with what we know\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "                axarr[axarr_index].scatter(x=ind[ind_index] + 0.125, y=mean, marker='s', s=8, c='b')\n",
    "                axarr[axarr_index].errorbar(x=ind[ind_index] + 0.125, y=mean, yerr=std, fmt='none', c='b')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    axarr[axarr_index].set_ylabel('', color='b')\n",
    "                    axarr[axarr_index].tick_params('y', colors='b')\n",
    "\n",
    "                    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "                    # axarr[axarr_index].spines['bottom'].set_visible(False)\n",
    "                    ax2.spines['top'].set_visible(False)\n",
    "                    # ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "                    axarr[axarr_index].set_xticks([a + 0.1875 for a in range(6)], minor=False)\n",
    "                    axarr[axarr_index].set_xticklabels(env_types_list)\n",
    "                    axarr[axarr_index].set_xlabel('Symbiodinium / post-MED')\n",
    "                    axarr[axarr_index].set_ylim((0, 1.1))\n",
    "\n",
    "                # PLOT UNIQUE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "\n",
    "                y_values = [tup[0] / tup[1] for tup in zip(list(env_QC_info_df.loc[:, 'post_med_unique']), list(\n",
    "                    env_QC_info_df.loc[:, 'post_taxa_id_unique_symbiodinium_seqs']))]\n",
    "                x_values = [ind[ind_index] + 0.250 for y in y_values]\n",
    "                ax2.scatter(x_values, y_values, marker='.', s=1, c='r')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "\n",
    "                ax2.scatter(x=ind[ind_index] + 0.375, y=mean, marker='o', s=8, c='r')\n",
    "                ax2.errorbar(x=ind[ind_index] + 0.375, y=mean, yerr=std, fmt='none', c='r')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    ax2.set_ylabel('', color='r')\n",
    "                    ax2.tick_params('y', colors='r')\n",
    "\n",
    "                ind_index += 1\n",
    "\n",
    "        axarr_index += 1\n",
    "    apples = 'asdf'\n",
    "    f.text(0, 0.65, 'ITS2 absolute sequence abundance', va='center', rotation='vertical', color='b')\n",
    "    f.text(1 - 0.01, 0.55, 'ITS2 unique sequence abundance', ha='center', va='center', rotation='vertical', color='r')\n",
    "    f.text(0.07, 0.18, 'ratio', va='center', rotation='vertical', color='b')\n",
    "    f.text(1 - 0.05, 0.18, 'ratio', va='center', rotation='vertical', color='r')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    f.savefig('diversity_stats.svg')\n",
    "    f.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversity stats all dates grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quan_diversity_figs_all_dates():\n",
    "    ''' This is a modification of the above so that all of the dates are considered.\n",
    "    The purpose of these figs will be to compare the diversity found in the different sample types\n",
    "    I envisage them being a sub plot each step of the QC:\n",
    "     1 - Post QC\n",
    "     2 - Symbiodinium\n",
    "     3 - non-Symbiodinium\n",
    "     4 - post - MED\n",
    "     5 - pst MED to pre MED symbiodinium ratio\n",
    "     number 5 will be important to consider and will hopefully hopefully be similar between all of the sample\n",
    "     types. It is important because MED has the possibility to collapse diverstiy in a non-linear manner.\n",
    "     e.g. in a sample with 1000 sequences, that has roughly the same spread as a sample with only 100 sequences\n",
    "     it is possible that MED will collapse both of these samples into 10 meaningful sequences. However, we would\n",
    "     then need to make sure to compare the different samples according to their pre-MED diversity. Although, we can\n",
    "     also discuss when the post-MED diversity means.\n",
    "     For each of teh sup plots I envisage there being a a 'set of plots' for each of the sample types\n",
    "     For each of these sets would in trun contain a set of two plots. One for absolute and one for unique. Each one\n",
    "     of these plots would be the actual datapoints on the left, and then a mean point with SD bars to the right of it\n",
    "     . We can put the absoulte and unique values on different axes as the differences between these will be huge.\n",
    "     We can achieve this by setting up subplot axes and then using the .errorbar and .scatter functions.\n",
    "     In terms of collecting the data we should simply use the cleaned up dataframes that we created when making the\n",
    "     diversity plots.'''\n",
    "\n",
    "    sp_output_df = pickle.load(open('sp_output_df_all_dates.pickle', 'rb'))\n",
    "    QC_info_df = pickle.load(open('QC_info_df_all_dates.pickle', 'rb'))\n",
    "    info_df = pickle.load(open('info_df_all_dates.pickle', 'rb'))\n",
    "\n",
    "    # remove sample P7-G07 as it has no Symbiodinium samples\n",
    "    sp_output_df.drop('P7_G07', axis='index', inplace=True)\n",
    "    QC_info_df.drop('P7_G07', axis='index', inplace=True)\n",
    "    info_df.drop('P7_G07', axis='index', inplace=True)\n",
    "\n",
    "    # lets make 5 subplot\n",
    "    # according to the above categories\n",
    "\n",
    "    f, axarr = plt.subplots(5, 1)\n",
    "    # counter to reference which set of axes we are plotting on\n",
    "    axarr_index = 0\n",
    "    # y_axis_labels = ['raw_contigs', 'post_qc', 'Symbiodinium', 'non-Symbiodinium', 'post-MED', 'post-MED / pre-MED']\n",
    "    y_axis_labels = ['raw_contigs',  'non-Symbiodinium','Symbiodinium',  'post-MED', 'post-MED / Symbiodinium']\n",
    "\n",
    "\n",
    "    # cycle through these strings to help us with our conditionals\n",
    "    # one of these for each of the subplots that we will create\n",
    "    # we will make these useful tuples that will hold the actual name of the columns that the data we want will\n",
    "    # be in so that we can pull these out of the dataframe easily\n",
    "    for sub_plot_type in [('raw_contigs',),\n",
    "                          ('post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs'),\n",
    "                          ('post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs'),\n",
    "                          ('post_med_absolute', 'post_med_unique'),\n",
    "                          ('med_ratio', True)]:\n",
    "\n",
    "\n",
    "        # for each of the sub plots we will want to grab the absolute and unique counts and plot these\n",
    "        # for each of the sample types.\n",
    "        # go environment type by environment type\n",
    "\n",
    "        # we will create some x axis indicies to arranage where we will be ploting\n",
    "        # we can be smart with these later on and create some nice spacing layouts but for the time\n",
    "        # being lets just get things plotted. Let's have one idices for each sample type and work\n",
    "        # relatively from there.\n",
    "        ind = range(6)\n",
    "        ind_index = 0\n",
    "\n",
    "\n",
    "        if sub_plot_type[0] != 'raw_contigs': ax2 = axarr[axarr_index].twinx()\n",
    "\n",
    "        axarr[axarr_index].set_xlabel(y_axis_labels[axarr_index])\n",
    "        env_types_list = ['coral', 'mucus', 'sea_water', 'sed_close', 'sed_far', 'turf']\n",
    "        for env_type in env_types_list:\n",
    "\n",
    "\n",
    "            if sub_plot_type[0] == 'raw_contigs':\n",
    "                # here we will plot just the raw_contigs\n",
    "                # get a sub df of the main df according to the env_type\n",
    "                # get subset of the main dfs that contain only the coral samples\n",
    "                env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "                env_QC_info_df = QC_info_df.loc[env_info_df.index.values.tolist()]\n",
    "                sys.stdout.write('\\nGenerating plotting info for {} samples in subplot type {}\\n'\n",
    "                                 .format(env_type, sub_plot_type))\n",
    "                # the data we are going to be plotting is so simple that rather than collecting it and then\n",
    "                # plotting it we may as well just go straight to plotting it from the df\n",
    "\n",
    "                # PLOT ABSOLUTE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "                y_values = list(env_QC_info_df.loc[:, sub_plot_type[0]])\n",
    "                x_values = [ind[ind_index] for y in y_values]\n",
    "                axarr[axarr_index].scatter(x_values, y_values, marker='.', s=1, c='b')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                # I know there is a mean and SD function on a pandas series but it is throwing out all sorts of\n",
    "                # erros so lest stick with what we know\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "                axarr[axarr_index].scatter(x=ind[ind_index] + 0.125, y=mean, marker='s', s=8, c='b')\n",
    "                axarr[axarr_index].errorbar(x=ind[ind_index] + 0.125, y=mean, yerr=std, fmt='none', c='b')\n",
    "\n",
    "                if env_type == 'turf':\n",
    "                    axarr[axarr_index].set_ylabel('', color='b')\n",
    "                    axarr[axarr_index].tick_params('y', colors='b')\n",
    "                    axarr[axarr_index].spines['right'].set_visible(False)\n",
    "                    # axarr[axarr_index].spines['bottom'].set_visible(False)\n",
    "                    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "\n",
    "                    # set the ticks\n",
    "                    # axarr[axarr_index].set_xticks([a + 0.1875 for a in range(6)], minor=False)\n",
    "                    # axarr[axarr_index].set_xticklabels(env_types_list)\n",
    "                    axarr[axarr_index].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "                    # set the xaxis title\n",
    "                    axarr[axarr_index].set_xlabel('raw_contigs')\n",
    "\n",
    "                    axarr[axarr_index].set_ylim((0, 1200000))\n",
    "\n",
    "                ind_index += 1\n",
    "            elif sub_plot_type[0] != 'med_ratio':\n",
    "                # get a sub df of the main df according to the env_type\n",
    "                # get subset of the main dfs that contain only the coral samples\n",
    "                env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "                env_QC_info_df = QC_info_df.loc[env_info_df.index.values.tolist()]\n",
    "                sys.stdout.write('\\nGenerating plotting info for {} samples in subplot type {}\\n'\n",
    "                                 .format(env_type, sub_plot_type))\n",
    "                # the data we are going to be plotting is so simple that rather than collecting it and then\n",
    "                # plotting it we may as well just go straight to plotting it from the df\n",
    "\n",
    "                # PLOT ABSOLUTE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "                y_values = list(env_QC_info_df.loc[:, sub_plot_type[0]])\n",
    "                x_values = [ind[ind_index] for y in y_values]\n",
    "                axarr[axarr_index].scatter(x_values, y_values, marker='.', s=1, c='b')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                # I know there is a mean and SD function on a pandas series but it is throwing out all sorts of\n",
    "                # erros so lest stick with what we know\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "                axarr[axarr_index].scatter(x=ind[ind_index] + 0.125, y=mean, marker='s', s=8, c='b')\n",
    "                axarr[axarr_index].errorbar(x=ind[ind_index] + 0.125, y=mean, yerr=std, fmt='none', c='b')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    axarr[axarr_index].set_ylabel('', color='b')\n",
    "                    axarr[axarr_index].tick_params('y', colors='b')\n",
    "\n",
    "\n",
    "                # PLOT UNIQUE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "\n",
    "                y_values = list(env_QC_info_df.loc[:, sub_plot_type[1]])\n",
    "                x_values = [ind[ind_index] + 0.250 for y in y_values]\n",
    "                ax2.scatter(x_values, y_values, marker='.', s=1, c='r')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "\n",
    "                ax2.scatter(x=ind[ind_index] + 0.375, y=mean, marker='o', s=8, c='r')\n",
    "                ax2.errorbar(x=ind[ind_index] + 0.375, y=mean, yerr=std, fmt='none', c='r')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    ax2.set_ylabel( '', color='r')\n",
    "                    ax2.tick_params('y', colors='r')\n",
    "                    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "                    # axarr[axarr_index].spines['bottom'].set_visible(False)\n",
    "                    ax2.spines['top'].set_visible(False)\n",
    "                    # ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "                    # axarr[axarr_index].set_xticks([a + 0.1875 for a in range(6)], minor=False)\n",
    "                    # axarr[axarr_index].set_xticklabels(env_types_list)\n",
    "                    axarr[axarr_index].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "                    ax2.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "                    axarr[axarr_index].set_xlabel(y_axis_labels[axarr_index])\n",
    "                    if sub_plot_type[0] == 'post_taxa_id_absolute_non_symbiodinium_seqs':\n",
    "                        axarr[axarr_index].set_ylim((0, 150000))\n",
    "                    elif sub_plot_type[0] == 'post_taxa_id_absolute_symbiodinium_seqs':\n",
    "                        axarr[axarr_index].set_ylim((0, 800000))\n",
    "                    elif sub_plot_type[0] == 'post_med_absolute':\n",
    "                        axarr[axarr_index].set_ylim((0, 800000))\n",
    "\n",
    "                ind_index += 1\n",
    "            else:\n",
    "                # here we need to ploto out the MED ratios.\n",
    "                # these are simply going to be the med abosultes divided by the symbiodinium absolutes\n",
    "                # and same for the uniques.\n",
    "                # get a sub df of the main df according to the env_type\n",
    "                # get subset of the main dfs that contain only the coral samples\n",
    "\n",
    "                env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "                env_QC_info_df = QC_info_df.loc[env_info_df.index.values.tolist()]\n",
    "                sys.stdout.write('\\nGenerating plotting info for {} samples in subplot type {}\\n'\n",
    "                                 .format(env_type, sub_plot_type))\n",
    "                # the data we are going to be plotting is so simple that rather than collecting it and then\n",
    "                # plotting it we may as well just go straight to plotting it from the df\n",
    "\n",
    "                # PLOT ABSOLUTE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "                y_values = [tup[0] / tup[1] for tup in zip(list(env_QC_info_df.loc[:, 'post_med_absolute']), list(\n",
    "                    env_QC_info_df.loc[:, 'post_taxa_id_absolute_symbiodinium_seqs']))]\n",
    "                x_values = [ind[ind_index] for y in y_values]\n",
    "                axarr[axarr_index].scatter(x_values, y_values, marker='.', s=1, c='b')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                # I know there is a mean and SD function on a pandas series but it is throwing out all sorts of\n",
    "                # erros so lest stick with what we know\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "                axarr[axarr_index].scatter(x=ind[ind_index] + 0.125, y=mean, marker='s', s=8, c='b')\n",
    "                axarr[axarr_index].errorbar(x=ind[ind_index] + 0.125, y=mean, yerr=std, fmt='none', c='b')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    axarr[axarr_index].set_ylabel('', color='b')\n",
    "                    axarr[axarr_index].tick_params('y', colors='b')\n",
    "\n",
    "                    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "                    # axarr[axarr_index].spines['bottom'].set_visible(False)\n",
    "                    ax2.spines['top'].set_visible(False)\n",
    "                    # ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "                    axarr[axarr_index].set_xticks([a + 0.1875 for a in range(6)], minor=False)\n",
    "                    axarr[axarr_index].set_xticklabels(env_types_list)\n",
    "                    axarr[axarr_index].set_xlabel('post-MED / Symbiodinium')\n",
    "                    axarr[axarr_index].set_ylim((0, 1.1))\n",
    "\n",
    "                # PLOT UNIQUE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "\n",
    "                y_values = [tup[0] / tup[1] for tup in zip(list(env_QC_info_df.loc[:, 'post_med_unique']), list(\n",
    "                    env_QC_info_df.loc[:, 'post_taxa_id_unique_symbiodinium_seqs']))]\n",
    "                x_values = [ind[ind_index] + 0.250 for y in y_values]\n",
    "                ax2.scatter(x_values, y_values, marker='.', s=1, c='r')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "\n",
    "                ax2.scatter(x=ind[ind_index] + 0.375, y=mean, marker='o', s=8, c='r')\n",
    "                ax2.errorbar(x=ind[ind_index] + 0.375, y=mean, yerr=std, fmt='none', c='r')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    ax2.set_ylabel('', color='r')\n",
    "                    ax2.tick_params('y', colors='r')\n",
    "\n",
    "                ind_index += 1\n",
    "\n",
    "        axarr_index += 1\n",
    "    apples = 'asdf'\n",
    "    f.text(0, 0.65, 'ITS2 absolute sequence abundance', va='center', rotation='vertical', color='b')\n",
    "    f.text(1 - 0.01, 0.55, 'ITS2 unique sequence abundance', ha='center', va='center', rotation='vertical', color='r')\n",
    "    f.text(0.07, 0.18, 'ratio', va='center', rotation='vertical', color='b')\n",
    "    f.text(1 - 0.05, 0.18, 'ratio', va='center', rotation='vertical', color='r')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    f.savefig('diversity_stats_all_dates.svg')\n",
    "    f.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversity stats all dates grouped and sed grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quan_diversity_figs_all_dates_sed_collapsed():\n",
    "    ''' This is a modification of the above so that all of the dates are considered.\n",
    "    The purpose of these figs will be to compare the diversity found in the different sample types\n",
    "    I envisage them being a sub plot each step of the QC:\n",
    "     1 - Post QC\n",
    "     2 - Symbiodinium\n",
    "     3 - non-Symbiodinium\n",
    "     4 - post - MED\n",
    "     5 - pst MED to pre MED symbiodinium ratio\n",
    "     number 5 will be important to consider and will hopefully hopefully be similar between all of the sample\n",
    "     types. It is important because MED has the possibility to collapse diverstiy in a non-linear manner.\n",
    "     e.g. in a sample with 1000 sequences, that has roughly the same spread as a sample with only 100 sequences\n",
    "     it is possible that MED will collapse both of these samples into 10 meaningful sequences. However, we would\n",
    "     then need to make sure to compare the different samples according to their pre-MED diversity. Although, we can\n",
    "     also discuss when the post-MED diversity means.\n",
    "     For each of teh sup plots I envisage there being a a 'set of plots' for each of the sample types\n",
    "     For each of these sets would in trun contain a set of two plots. One for absolute and one for unique. Each one\n",
    "     of these plots would be the actual datapoints on the left, and then a mean point with SD bars to the right of it\n",
    "     . We can put the absoulte and unique values on different axes as the differences between these will be huge.\n",
    "     We can achieve this by setting up subplot axes and then using the .errorbar and .scatter functions.\n",
    "     In terms of collecting the data we should simply use the cleaned up dataframes that we created when making the\n",
    "     diversity plots.'''\n",
    "\n",
    "    sp_output_df = pickle.load(open('sp_output_df_all_dates.pickle', 'rb'))\n",
    "    QC_info_df = pickle.load(open('QC_info_df_all_dates.pickle', 'rb'))\n",
    "    info_df = pickle.load(open('info_df_all_dates.pickle', 'rb'))\n",
    "\n",
    "    # remove sample P7-G07 as it has no Symbiodinium samples\n",
    "    sp_output_df.drop('P7_G07', axis='index', inplace=True)\n",
    "    QC_info_df.drop('P7_G07', axis='index', inplace=True)\n",
    "    info_df.drop('P7_G07', axis='index', inplace=True)\n",
    "\n",
    "    # lets make 5 subplot\n",
    "    # according to the above categories\n",
    "\n",
    "    f, axarr = plt.subplots(5, 1, figsize=(10,8))\n",
    "    # counter to reference which set of axes we are plotting on\n",
    "    axarr_index = 0\n",
    "    # y_axis_labels = ['raw_contigs', 'post_qc', 'Symbiodinium', 'non-Symbiodinium', 'post-MED', 'post-MED / pre-MED']\n",
    "    y_axis_labels = ['raw_contigs',  'non-Symbiodinium','Symbiodinium',  'post-MED', 'post-MED / Symbiodinium']\n",
    "\n",
    "\n",
    "    # cycle through these strings to help us with our conditionals\n",
    "    # one of these for each of the subplots that we will create\n",
    "    # we will make these useful tuples that will hold the actual name of the columns that the data we want will\n",
    "    # be in so that we can pull these out of the dataframe easily\n",
    "    for sub_plot_type in [('raw_contigs',),\n",
    "                          ('post_taxa_id_absolute_non_symbiodinium_seqs', 'post_taxa_id_unique_non_symbiodinium_seqs'),\n",
    "                          ('post_taxa_id_absolute_symbiodinium_seqs', 'post_taxa_id_unique_symbiodinium_seqs'),\n",
    "                          ('post_med_absolute', 'post_med_unique'),\n",
    "                          ('med_ratio', True)]:\n",
    "\n",
    "\n",
    "        # for each of the sub plots we will want to grab the absolute and unique counts and plot these\n",
    "        # for each of the sample types.\n",
    "        # go environment type by environment type\n",
    "\n",
    "        # we will create some x axis indicies to arranage where we will be ploting\n",
    "        # we can be smart with these later on and create some nice spacing layouts but for the time\n",
    "        # being lets just get things plotted. Let's have one idices for each sample type and work\n",
    "        # relatively from there.\n",
    "        ind = range(5)\n",
    "        ind_index = 0\n",
    "\n",
    "\n",
    "        if sub_plot_type[0] != 'raw_contigs': ax2 = axarr[axarr_index].twinx()\n",
    "\n",
    "        axarr[axarr_index].set_xlabel(y_axis_labels[axarr_index])\n",
    "\n",
    "        # we will convert the sed_close and sed_far to simply sed\n",
    "        info_df = info_df.applymap(lambda x: 'sed' if \"sed\" in str(x) else x)\n",
    "        env_types_list = ['coral', 'mucus', 'sea_water', 'sed', 'turf']\n",
    "        for env_type in env_types_list:\n",
    "\n",
    "\n",
    "            if sub_plot_type[0] == 'raw_contigs':\n",
    "                # here we will plot just the raw_contigs\n",
    "                # get a sub df of the main df according to the env_type\n",
    "                # get subset of the main dfs that contain only the coral samples\n",
    "                env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "                env_QC_info_df = QC_info_df.loc[env_info_df.index.values.tolist()]\n",
    "                sys.stdout.write('\\nGenerating plotting info for {} samples in subplot type {}\\n'\n",
    "                                 .format(env_type, sub_plot_type))\n",
    "                # the data we are going to be plotting is so simple that rather than collecting it and then\n",
    "                # plotting it we may as well just go straight to plotting it from the df\n",
    "\n",
    "                # PLOT ABSOLUTE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "                y_values = list(env_QC_info_df.loc[:, sub_plot_type[0]])\n",
    "                x_values = [ind[ind_index] for y in y_values]\n",
    "                axarr[axarr_index].scatter(x_values, y_values, marker='.', s=1, c='b')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                # I know there is a mean and SD function on a pandas series but it is throwing out all sorts of\n",
    "                # erros so lest stick with what we know\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "                axarr[axarr_index].scatter(x=ind[ind_index] + 0.125, y=mean, marker='s', s=8, c='b')\n",
    "                axarr[axarr_index].errorbar(x=ind[ind_index] + 0.125, y=mean, yerr=std, fmt='none', c='b')\n",
    "\n",
    "                if env_type == 'turf':\n",
    "                    axarr[axarr_index].set_ylabel('', color='b')\n",
    "                    axarr[axarr_index].tick_params('y', colors='b')\n",
    "                    axarr[axarr_index].spines['right'].set_visible(False)\n",
    "                    # axarr[axarr_index].spines['bottom'].set_visible(False)\n",
    "                    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "\n",
    "                    # set the ticks\n",
    "                    # axarr[axarr_index].set_xticks([a + 0.1875 for a in range(6)], minor=False)\n",
    "                    # axarr[axarr_index].set_xticklabels(env_types_list)\n",
    "                    axarr[axarr_index].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "                    # set the xaxis title\n",
    "                    axarr[axarr_index].set_xlabel('raw_contigs')\n",
    "\n",
    "                    axarr[axarr_index].set_ylim((0, 1200000))\n",
    "\n",
    "                ind_index += 1\n",
    "            elif sub_plot_type[0] != 'med_ratio':\n",
    "                # get a sub df of the main df according to the env_type\n",
    "                # get subset of the main dfs that contain only the coral samples\n",
    "                env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "                env_QC_info_df = QC_info_df.loc[env_info_df.index.values.tolist()]\n",
    "                sys.stdout.write('\\nGenerating plotting info for {} samples in subplot type {}\\n'\n",
    "                                 .format(env_type, sub_plot_type))\n",
    "                # the data we are going to be plotting is so simple that rather than collecting it and then\n",
    "                # plotting it we may as well just go straight to plotting it from the df\n",
    "\n",
    "                # PLOT ABSOLUTE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "                y_values = list(env_QC_info_df.loc[:, sub_plot_type[0]])\n",
    "                x_values = [ind[ind_index] for y in y_values]\n",
    "                axarr[axarr_index].scatter(x_values, y_values, marker='.', s=1, c='b')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                # I know there is a mean and SD function on a pandas series but it is throwing out all sorts of\n",
    "                # erros so lest stick with what we know\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "                axarr[axarr_index].scatter(x=ind[ind_index] + 0.125, y=mean, marker='s', s=8, c='b')\n",
    "                axarr[axarr_index].errorbar(x=ind[ind_index] + 0.125, y=mean, yerr=std, fmt='none', c='b')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    axarr[axarr_index].set_ylabel('', color='b')\n",
    "                    axarr[axarr_index].tick_params('y', colors='b')\n",
    "\n",
    "\n",
    "                # PLOT UNIQUE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "\n",
    "                y_values = list(env_QC_info_df.loc[:, sub_plot_type[1]])\n",
    "                x_values = [ind[ind_index] + 0.250 for y in y_values]\n",
    "                ax2.scatter(x_values, y_values, marker='.', s=1, c='r')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "\n",
    "                ax2.scatter(x=ind[ind_index] + 0.375, y=mean, marker='o', s=8, c='r')\n",
    "                ax2.errorbar(x=ind[ind_index] + 0.375, y=mean, yerr=std, fmt='none', c='r')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    ax2.set_ylabel( '', color='r')\n",
    "                    ax2.tick_params('y', colors='r')\n",
    "                    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "                    # axarr[axarr_index].spines['bottom'].set_visible(False)\n",
    "                    ax2.spines['top'].set_visible(False)\n",
    "                    # ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "                    # axarr[axarr_index].set_xticks([a + 0.1875 for a in range(6)], minor=False)\n",
    "                    # axarr[axarr_index].set_xticklabels(env_types_list)\n",
    "                    axarr[axarr_index].tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "                    ax2.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "                    axarr[axarr_index].set_xlabel(y_axis_labels[axarr_index])\n",
    "                    if sub_plot_type[0] == 'post_taxa_id_absolute_non_symbiodinium_seqs':\n",
    "                        axarr[axarr_index].set_ylim((0, 150000))\n",
    "                    elif sub_plot_type[0] == 'post_taxa_id_absolute_symbiodinium_seqs':\n",
    "                        axarr[axarr_index].set_ylim((0, 800000))\n",
    "                    elif sub_plot_type[0] == 'post_med_absolute':\n",
    "                        axarr[axarr_index].set_ylim((0, 800000))\n",
    "\n",
    "                ind_index += 1\n",
    "            else:\n",
    "                # here we need to ploto out the MED ratios.\n",
    "                # these are simply going to be the med abosultes divided by the symbiodinium absolutes\n",
    "                # and same for the uniques.\n",
    "                # get a sub df of the main df according to the env_type\n",
    "                # get subset of the main dfs that contain only the coral samples\n",
    "\n",
    "                env_info_df = info_df[info_df['environ type'] == env_type]\n",
    "                env_QC_info_df = QC_info_df.loc[env_info_df.index.values.tolist()]\n",
    "                sys.stdout.write('\\nGenerating plotting info for {} samples in subplot type {}\\n'\n",
    "                                 .format(env_type, sub_plot_type))\n",
    "                # the data we are going to be plotting is so simple that rather than collecting it and then\n",
    "                # plotting it we may as well just go straight to plotting it from the df\n",
    "\n",
    "                # PLOT ABSOLUTE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "                y_values = [tup[0] / tup[1] for tup in zip(list(env_QC_info_df.loc[:, 'post_med_absolute']), list(\n",
    "                    env_QC_info_df.loc[:, 'post_taxa_id_absolute_symbiodinium_seqs']))]\n",
    "                x_values = [ind[ind_index] for y in y_values]\n",
    "                axarr[axarr_index].scatter(x_values, y_values, marker='.', s=1, c='b')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                # I know there is a mean and SD function on a pandas series but it is throwing out all sorts of\n",
    "                # erros so lest stick with what we know\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "                axarr[axarr_index].scatter(x=ind[ind_index] + 0.125, y=mean, marker='s', s=8, c='b')\n",
    "                axarr[axarr_index].errorbar(x=ind[ind_index] + 0.125, y=mean, yerr=std, fmt='none', c='b')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    axarr[axarr_index].set_ylabel('', color='b')\n",
    "                    axarr[axarr_index].tick_params('y', colors='b')\n",
    "\n",
    "                    axarr[axarr_index].spines['top'].set_visible(False)\n",
    "                    # axarr[axarr_index].spines['bottom'].set_visible(False)\n",
    "                    ax2.spines['top'].set_visible(False)\n",
    "                    # ax2.spines['bottom'].set_visible(False)\n",
    "\n",
    "                    axarr[axarr_index].set_xticks([a + 0.1875 for a in range(6)], minor=False)\n",
    "                    axarr[axarr_index].set_xticklabels(env_types_list)\n",
    "                    axarr[axarr_index].set_xlabel('post-MED / Symbiodinium')\n",
    "                    axarr[axarr_index].set_ylim((0, 1.1))\n",
    "\n",
    "                # PLOT UNIQUE\n",
    "                # first plot the actual datapoints\n",
    "                # x will be the indices, y will be the actual value\n",
    "\n",
    "                y_values = [tup[0] / tup[1] for tup in zip(list(env_QC_info_df.loc[:, 'post_med_unique']), list(\n",
    "                    env_QC_info_df.loc[:, 'post_taxa_id_unique_symbiodinium_seqs']))]\n",
    "                x_values = [ind[ind_index] + 0.250 for y in y_values]\n",
    "                ax2.scatter(x_values, y_values, marker='.', s=1, c='r')\n",
    "\n",
    "                # now plot the mean and error bars\n",
    "                std = statistics.stdev(y_values)\n",
    "                mean = statistics.mean(y_values)\n",
    "\n",
    "                ax2.scatter(x=ind[ind_index] + 0.375, y=mean, marker='o', s=8, c='r')\n",
    "                ax2.errorbar(x=ind[ind_index] + 0.375, y=mean, yerr=std, fmt='none', c='r')\n",
    "\n",
    "                if env_type == 'coral':\n",
    "                    ax2.set_ylabel('', color='r')\n",
    "                    ax2.tick_params('y', colors='r')\n",
    "\n",
    "                ind_index += 1\n",
    "\n",
    "        axarr_index += 1\n",
    "    apples = 'asdf'\n",
    "    f.text(0, 0.65, 'ITS2 absolute sequence abundance', va='center', rotation='vertical', color='b')\n",
    "    f.text(1 - 0.01, 0.55, 'ITS2 unique sequence abundance', ha='center', va='center', rotation='vertical', color='r')\n",
    "    f.text(0.07, 0.18, 'ratio', va='center', rotation='vertical', color='b')\n",
    "    f.text(1 - 0.05, 0.18, 'ratio', va='center', rotation='vertical', color='r')\n",
    "\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    f.savefig('diversity_stats_all_dates_sed_grouped.svg')\n",
    "    f.savefig('diversity_stats_all_dates_sed_grouped.png')\n",
    "    f.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venn one date no abund labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_venns():\n",
    "    ''' Here we will aim to make venn diagrams we will make a set of three 2 cirle venns which will\n",
    "    just show the comparison of the env_samples to the coral.\n",
    "    We will also make a set of four 3 circle venn diagrams which will show all 3 way combinations.\n",
    "    There is a neat little module called matplotlib_venn which we can use to do this and it is dead simple.\n",
    "    All you need to do is pass it a list of sets.'''\n",
    "\n",
    "    # read in the dataframes created previously\n",
    "    sp_output_df = pickle.load(open('sp_output_df.pickle', 'rb'))\n",
    "    QC_info_df = pickle.load(open('QC_info_df.pickle', 'rb'))\n",
    "    info_df = pickle.load(open('info_df.pickle', 'rb'))\n",
    "\n",
    "    # create a dictionary that is sample name to env_type\n",
    "    sample_name_to_sample_type_dict = {}\n",
    "    for info_index in info_df.index.values.tolist():\n",
    "        if info_df.loc[info_index, 'environ type'] == 'coral':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'coral'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'sea_water':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'sea_water'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'sed_far':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'sed'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'sed_close':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'sed'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'mucus':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'mucus'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'turf':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'turf'\n",
    "\n",
    "    # here we have the dict populated and we can now get to work pulling out the set of sequence\n",
    "    # names found in each of the sample types\n",
    "    sample_types = ['coral', 'sea_water', 'sed', 'mucus', 'turf']\n",
    "\n",
    "    set_list = [set() for type in sample_types]\n",
    "\n",
    "    # now work through the sp output and check to see which columns are non-zero columns and add these column\n",
    "    # labels into the respective sets\n",
    "    for sp_output_index in sp_output_df.index.values.tolist():\n",
    "        type_of_sample  = sample_name_to_sample_type_dict[sp_output_index]\n",
    "        non_zero_seqs = list(sp_output_df.loc[sp_output_index][sp_output_df.loc[sp_output_index] > 0].index)\n",
    "        set_list[sample_types.index(type_of_sample)].update(non_zero_seqs)\n",
    "\n",
    "    # here we should have the sets populated\n",
    "    # now create a dictionary that is the sample_type name to the set\n",
    "    sample_type_set_dict = {}\n",
    "    for smp_t in sample_types:\n",
    "        sample_type_set_dict[smp_t] = (smp_t, set_list[sample_types.index(smp_t)])\n",
    "\n",
    "    colour_dict = {\n",
    "        'coral' : 'orange',\n",
    "        'sed' : 'brown',\n",
    "        'sea_water' : 'blue',\n",
    "        'turf' : 'green',\n",
    "        'mucus' : 'gray'}\n",
    "\n",
    "\n",
    "    figure, axes = plt.subplots(2, 2)\n",
    "    ax_count = 0\n",
    "    # then simply do permutations for the 5 x 3\n",
    "\n",
    "    coral_combo_plot_list = [('coral', 'mucus'), ('coral', 'sea_water'), ('coral', 'turf'), ('coral', 'sed')]\n",
    "    for combo in coral_combo_plot_list:\n",
    "        set_list = [sample_type_set_dict[combo[0]][1], sample_type_set_dict[combo[1]][1]]\n",
    "        labels = [sample_type_set_dict[combo[0]][0], sample_type_set_dict[combo[1]][0]]\n",
    "        c = venn2(subsets=set_list, set_labels=labels, ax=axes[int(ax_count / 2)][ax_count % 2])\n",
    "        element_list = ['10', '01']\n",
    "        for i in range(2):\n",
    "            # for each path we need to work out which colour we want it to be\n",
    "            # we can do this with a simple dict outside of here\n",
    "            c.get_patch_by_id(element_list[i]).set_color(colour_dict[labels[i]])\n",
    "            c.get_patch_by_id(element_list[i]).set_edgecolor('none')\n",
    "            c.get_patch_by_id(element_list[i]).set_alpha(0.4)\n",
    "        ax_count += 1\n",
    "\n",
    "\n",
    "\n",
    "    figure.savefig('coral_combo_venn.svg')\n",
    "    figure.show()\n",
    "    plt.close()\n",
    "\n",
    "    figure, axes = plt.subplots(1, 1)\n",
    "    # now lets plot the other vann of the three env_types against each other\n",
    "    set_list = [sample_type_set_dict['sea_water'][1], sample_type_set_dict['turf'][1], sample_type_set_dict['sed'][1]]\n",
    "    labels = [sample_type_set_dict['sea_water'][0], sample_type_set_dict['turf'][0], sample_type_set_dict['sed'][0]]\n",
    "\n",
    "    c = venn3(subsets=set_list, set_labels=labels, ax=axes)\n",
    "    element_list = ['100', '010', '001']\n",
    "    for i in range(3):\n",
    "        # for each path we need to work out which colour we want it to be\n",
    "        # we can do this with a simple dict outside of here\n",
    "        c.get_patch_by_id(element_list[i]).set_color(colour_dict[labels[i]])\n",
    "        c.get_patch_by_id(element_list[i]).set_edgecolor('none')\n",
    "        c.get_patch_by_id(element_list[i]).set_alpha(0.4)\n",
    "\n",
    "    figure.savefig('env_combo_venn.svg')\n",
    "    figure.show()\n",
    "    apples = 'adf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venn all dates with abund labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_venns_all_dates():\n",
    "    ''' This is a modification of the above the only difference being that we are working with all samples\n",
    "    of all times. It would be nice to somehow represent what proportion of the total sequences are represented\n",
    "    by the unique samples but I don't actually think this is possible. Instead we will create labels that\n",
    "    show what proportion of the total sequences the unique sequences represent.\n",
    "    Here we will aim to make venn diagrams we will make a set of three 2 cirle venns which will\n",
    "    just show the comparison of the env_samples to the coral.\n",
    "    We will also make a set of four 3 circle venn diagrams which will show all 3 way combinations.\n",
    "    There is a neat little module called matplotlib_venn which we can use to do this and it is dead simple.\n",
    "    All you need to do is pass it a list of sets.'''\n",
    "\n",
    "    # read in the dataframes created previously\n",
    "    sp_output_df = pickle.load(open('sp_output_df_all_dates.pickle', 'rb'))\n",
    "    QC_info_df = pickle.load(open('QC_info_df_all_dates.pickle', 'rb'))\n",
    "    info_df = pickle.load(open('info_df_all_dates.pickle', 'rb'))\n",
    "\n",
    "\n",
    "    # in order to be able to calculate the proportion of a sample types sequencs that the various sequence regions\n",
    "    # represent in proportion to the total sequences we need to work with a prportion df\n",
    "    sp_output_df = sp_output_df[:].div(sp_output_df[:].sum(axis=1), axis=0)\n",
    "\n",
    "    # create a dictionary that is sample name to env_type\n",
    "    sample_name_to_sample_type_dict = {}\n",
    "    for info_index in info_df.index.values.tolist():\n",
    "        if info_df.loc[info_index, 'environ type'] == 'coral':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'coral'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'sea_water':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'sea_water'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'sed_far':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'sed'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'sed_close':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'sed'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'mucus':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'mucus'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'turf':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'turf'\n",
    "\n",
    "    # here we have the dict populated and we can now get to work pulling out the set of sequence\n",
    "    # names found in each of the sample types\n",
    "    sample_types = ['coral', 'sea_water', 'sed', 'mucus', 'turf']\n",
    "\n",
    "\n",
    "    set_list = [set() for type in sample_types]\n",
    "\n",
    "    # now work through the sp output and check to see which columns are non-zero columns and add these column\n",
    "    # labels into the respective sets\n",
    "    # here we will create a list of dictionaries, one per sample type and this will be\n",
    "    # key = sequence, value = cumulative rel_props\n",
    "    for sp_output_index in sp_output_df.index.values.tolist():\n",
    "        type_of_sample = sample_name_to_sample_type_dict[sp_output_index]\n",
    "        non_zero_seqs = list(sp_output_df.loc[sp_output_index][sp_output_df.loc[sp_output_index] > 0].index)\n",
    "        set_list[sample_types.index(type_of_sample)].update(non_zero_seqs)\n",
    "\n",
    "    # here we should have the sets populated\n",
    "    # now create a dictionary that is the sample_type name to the set\n",
    "    sample_type_set_dict = {}\n",
    "    for smp_t in sample_types:\n",
    "        sample_type_set_dict[smp_t] = (smp_t, set_list[sample_types.index(smp_t)])\n",
    "\n",
    "    colour_dict = {\n",
    "        'coral': 'orange',\n",
    "        'sed': 'brown',\n",
    "        'sea_water': 'blue',\n",
    "        'turf': 'green',\n",
    "        'mucus': 'gray'}\n",
    "\n",
    "    figure, axes = plt.subplots(2, 2)\n",
    "    ax_count = 0\n",
    "    # then simply do permutations for the 5 x 3\n",
    "\n",
    "    coral_combo_plot_list = [('coral', 'mucus'), ('coral', 'sea_water'), ('coral', 'turf'), ('coral', 'sed')]\n",
    "    for combo in coral_combo_plot_list:\n",
    "        set_list = [sample_type_set_dict[combo[0]][1], sample_type_set_dict[combo[1]][1]]\n",
    "        labels = [sample_type_set_dict[combo[0]][0], sample_type_set_dict[combo[1]][0]]\n",
    "        c = venn2(subsets=set_list, set_labels=labels, ax=axes[int(ax_count / 2)][ax_count % 2])\n",
    "        element_list = ['10', '01']\n",
    "        for i in range(2):\n",
    "            # for each path we need to work out which colour we want it to be\n",
    "            # we can do this with a simple dict outside of here\n",
    "            c.get_patch_by_id(element_list[i]).set_color(colour_dict[labels[i]])\n",
    "            c.get_patch_by_id(element_list[i]).set_edgecolor('none')\n",
    "            c.get_patch_by_id(element_list[i]).set_alpha(0.4)\n",
    "        ax_count += 1\n",
    "\n",
    "    figure.savefig('coral_combo_venn_all_dates.svg')\n",
    "    figure.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    figure, axes = plt.subplots(1, 1)\n",
    "    # now lets plot the other vann of the three env_types against each other\n",
    "    set_list = [sample_type_set_dict['sea_water'][1], sample_type_set_dict['turf'][1],\n",
    "                sample_type_set_dict['sed'][1]]\n",
    "    labels = [sample_type_set_dict['sea_water'][0], sample_type_set_dict['turf'][0], sample_type_set_dict['sed'][0]]\n",
    "\n",
    "    c = venn3(subsets=set_list, set_labels=labels, ax=axes)\n",
    "    element_list = ['100', '010', '001']\n",
    "    for i in range(3):\n",
    "        # for each path we need to work out which colour we want it to be\n",
    "        # we can do this with a simple dict outside of here\n",
    "        c.get_patch_by_id(element_list[i]).set_color(colour_dict[labels[i]])\n",
    "        c.get_patch_by_id(element_list[i]).set_edgecolor('none')\n",
    "        c.get_patch_by_id(element_list[i]).set_alpha(0.4)\n",
    "\n",
    "    figure.savefig('env_combo_venn_all_dates.svg')\n",
    "    figure.show()\n",
    "    apples = 'adf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rarefaction curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rarefaction_curve_worker(input_queue, num_bootstraps, result_dict, sampling_frequencies):\n",
    "\n",
    "    for name, working_dict in iter(input_queue.get, 'STOP'):\n",
    "\n",
    "        sys.stdout.write('\\n\\nSample: {}'.format(name))\n",
    "        # for each sample, perform the bootstrapping\n",
    "\n",
    "        # the enormous list that will hold a non-redundant list of sequences for picking from\n",
    "        picking_list = []\n",
    "        for non_zero_col_labl in working_dict.keys():\n",
    "            picking_list.extend([non_zero_col_labl for i in range(working_dict[non_zero_col_labl])])\n",
    "\n",
    "        # this will hold the lists which will hold the results of a single bool\n",
    "        # so this will hold the results of all of the bools\n",
    "        sample_boot_result_holder = []\n",
    "        for it_num in range(num_bootstraps):\n",
    "            sys.stdout.write('\\nbootstrap: {}\\n'.format(it_num))\n",
    "            # this is the set that we will populate to count number of unique seqs\n",
    "            sample_result_holder = []\n",
    "            temp_set = set()\n",
    "            try:\n",
    "                pick_array = np.random.choice(picking_list, len(picking_list), replace=False)\n",
    "            except:\n",
    "                asdf = 'asdf'\n",
    "            for i in range(len(pick_array)):\n",
    "                sys.stdout.write('\\rbootstrap: {}'.format(i))\n",
    "                temp_set.add(pick_array[i])\n",
    "                if i in sampling_frequencies:\n",
    "                    # then we sample the length of the set\n",
    "                    sample_result_holder.append(len(temp_set))\n",
    "            sample_boot_result_holder.append(sample_result_holder)\n",
    "\n",
    "        # here we have conducted the bootstrapping for one of the samples\n",
    "        result_dict[name] = sample_boot_result_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rarefaction_curves():\n",
    "    ''' The purpose of this will be to create a figure that has a rarefaction curve for each of the sample types.\n",
    "    I'm hoping that this will do an excellent job of mitigating the problem we currently have with regards to the\n",
    "    differences in sequences returned from the different environemtns.\n",
    "    We can look up a paper that supports the fact that relatively accurate inference can be made\n",
    "    using the begining part of the curve.\n",
    "\n",
    "    To draw the rarefaction curve we will work on a sample by sample basis. For each sample, we will create a list\n",
    "    that contains the resundant seuqnces according to aboslute counts (massive list) and then simply do random\n",
    "    selects on this without replacement to get an array of sequences in the pick order. We can then iterate through\n",
    "    this picked order and put the seqs into a set. We can then get the len of the set at given intervlas. the intercal\n",
    "    vs. the set len will be our coordinates for the plotting. However, we will bootstrap this so we will do the above\n",
    "    process per sample maybe 100 times. then for each of the intervals we can get an average. Then for all of the\n",
    "    sampes of a given sample type we can average all of the diversity points at each interval and plot this point\n",
    "    (maybe we also plot the individual points too). Obviously we will lose points as we work along our intervals.\n",
    "    This is fine (and an inherent part of the data which we are precielcy trying to overcome with this rarefaction\n",
    "    approach) but we should make the reader aware of this decrease by having an n on the graph. Also error bars would\n",
    "    be nice. This may mean that we need to have different subplots for each of the curves though.\n",
    "    '''\n",
    "\n",
    "    # read in the dataframes\n",
    "    sp_output_df = pickle.load(open('sp_output_df_all_dates.pickle', 'rb'))\n",
    "    QC_info_df = pickle.load(open('QC_info_df_all_dates.pickle', 'rb'))\n",
    "    info_df = pickle.load(open('info_df_all_dates.pickle', 'rb'))\n",
    "\n",
    "    # remove sample P7-G07 as it has no Symbiodinium samples\n",
    "    sp_output_df.drop('P7_G07', axis='index', inplace=True)\n",
    "    QC_info_df.drop('P7_G07', axis='index', inplace=True)\n",
    "    info_df.drop('P7_G07', axis='index', inplace=True)\n",
    "    # remove samples that don't have any counts in the MED QC columns\n",
    "    keeper_row_labels = QC_info_df.index[QC_info_df['post_med_absolute'] != 0]\n",
    "    QC_info_df = QC_info_df.loc[keeper_row_labels]\n",
    "    sp_output_df = sp_output_df.loc[keeper_row_labels]\n",
    "    info_df = info_df.loc[keeper_row_labels]\n",
    "    apples = 'asdf'\n",
    "\n",
    "    # create a dictionary that is sample name to env_type\n",
    "    sample_name_to_sample_type_dict = {}\n",
    "    for info_index in info_df.index.values.tolist():\n",
    "        if info_df.loc[info_index, 'environ type'] == 'coral':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'coral'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'sea_water':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'sea_water'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'sed_far':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'sed'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'sed_close':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'sed'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'mucus':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'mucus'\n",
    "        elif info_df.loc[info_index, 'environ type'] == 'turf':\n",
    "            sample_name_to_sample_type_dict[info_index] = 'turf'\n",
    "\n",
    "\n",
    "    boot_iterations = 100\n",
    "\n",
    "    # get a list of the sampling frequencies that we want to sample over\n",
    "    sampling_frequencies = []\n",
    "    additions_list = [0, 0.25, 0.5, 0.75]\n",
    "    orders = range(1, 6)\n",
    "    for order in orders:\n",
    "        for addition in additions_list:\n",
    "            sampling_frequencies.append(int(10 ** (order + addition)))\n",
    "\n",
    "    if os.path.isfile('result_dict_rare_{}.pickle'.format(boot_iterations)):\n",
    "        result_dict = pickle.load(open('result_dict_rare_{}.pickle'.format(boot_iterations), 'rb'))\n",
    "        apples = 'asdf'\n",
    "    else:\n",
    "\n",
    "        # get a list of the sampling frequencies that we want to sample over\n",
    "        sampling_frequencies = []\n",
    "        additions_list = [0, 0.25, 0.5, 0.75]\n",
    "        orders = range(1, 6)\n",
    "        for order in orders:\n",
    "            for addition in additions_list:\n",
    "                sampling_frequencies.append(int(10 ** (order + addition)))\n",
    "\n",
    "        # we will send one series to be bootstrapped to a core for MPing\n",
    "        input_series_queue = Queue()\n",
    "\n",
    "        num_proc = 26\n",
    "\n",
    "        manager = Manager()\n",
    "        result_dict = manager.dict()\n",
    "\n",
    "        for smp_index in sp_output_df.index.values.tolist():\n",
    "            sys.stdout.write('\\rPrinting: {}'.format(smp_index))\n",
    "            non_zero_indices = list(sp_output_df.loc[smp_index].nonzero()[0])\n",
    "            non_zero_series = sp_output_df.loc[smp_index].iloc[non_zero_indices]\n",
    "\n",
    "            with open('test_bob.txt', 'w') as f:\n",
    "                for item in non_zero_series.index.values:\n",
    "                    f.write('{}\\n'.format(item))\n",
    "            new_list = []\n",
    "            with open('test_bob.txt', 'r') as f:\n",
    "                new_list.extend([line.rstrip() for line in f])\n",
    "            new_dict = {a:b for a, b, in zip(new_list, non_zero_series.values.tolist())}\n",
    "\n",
    "            input_series_queue.put((smp_index, new_dict))\n",
    "\n",
    "        for i in range(num_proc):\n",
    "            input_series_queue.put('STOP')\n",
    "\n",
    "        list_of_processes = []\n",
    "        for N in range(num_proc):\n",
    "            # p = Process(target=rarefaction_curve_worker, args=(input_series_queue, boot_iterations,\n",
    "            #                                                    result_dict, sampling_frequencies))\n",
    "            p = Process(target=rarefaction_curve_worker, args=(input_series_queue, boot_iterations,\n",
    "                                                                result_dict, sampling_frequencies))\n",
    "            list_of_processes.append(p)\n",
    "            p.start()\n",
    "\n",
    "        for p in list_of_processes:\n",
    "            p.join()\n",
    "\n",
    "        pickle.dump(dict(result_dict), open('result_dict_rare_{}.pickle'.format(boot_iterations), 'wb'))\n",
    "\n",
    "    # we have our results that we can work with held in the result_dict\n",
    "    # we should be able to work directly with this dictionary for plotting so lets set this up now\n",
    "\n",
    "    colour_dict = {\n",
    "        'coral': 'orange',\n",
    "        'sed': 'brown',\n",
    "        'sea_water': 'blue',\n",
    "        'turf': 'green',\n",
    "        'mucus': 'gray'}\n",
    "\n",
    "    fig, axarr = plt.subplots(1, 6, sharey=True, sharex=True, figsize=(10,6))\n",
    "\n",
    "    # axarr[0].set_xscale('log')\n",
    "\n",
    "    axx_ind = 0\n",
    "    env_type_list = ['coral', 'mucus', 'sea_water', 'sed', 'turf']\n",
    "\n",
    "    # Dict that will hold a list of the averaged series grouped by the sample type\n",
    "    data_info = defaultdict(list)\n",
    "\n",
    "    # for each sample workout the means of the bottstraps as a series and add these to the dict\n",
    "    for smp in result_dict.keys():\n",
    "        env_type_of_smp = sample_name_to_sample_type_dict[smp]\n",
    "        temp_df = pd.DataFrame(result_dict[smp])\n",
    "        averaged_series = temp_df.mean(axis=0)\n",
    "        averaged_series.name = smp\n",
    "        data_info[env_type_of_smp].append(averaged_series)\n",
    "\n",
    "\n",
    "    # here we should have all of the info we need to make a dataframe that can directly be used for plotting for\n",
    "    # each of the environmental sample types\n",
    "\n",
    "    # This will hold the pairs of x, y lists for each of the environment types so that we can plot them all on the\n",
    "    # last plot\n",
    "    line_holder = []\n",
    "    for env_type in env_type_list:\n",
    "        try:\n",
    "            # env_type_df = pd.DataFrame(data_info[env_type], columns = sampling_frequencies)\n",
    "            env_type_df = pd.DataFrame.from_items([(s.name, s) for s in data_info[env_type]]).T\n",
    "        except:\n",
    "            asdf = 'asdf'\n",
    "\n",
    "        # here we need to add the columns to the df.\n",
    "        # if we had points for all of the sampling frequencies then we can simply use the sampling frequency values\n",
    "        # else we need to use a slice of the sampling_frequencies\n",
    "        if len(list(env_type_df)) == len(sampling_frequencies):\n",
    "            env_type_df.columns = sampling_frequencies\n",
    "        else:\n",
    "            env_type_df.columns = sampling_frequencies[:len(list(env_type_df))]\n",
    "\n",
    "        mean_y = []\n",
    "        mean_x = []\n",
    "        num_samples = len(env_type_df.iloc[:,0])\n",
    "        for col in list(env_type_df):\n",
    "            # here plot the individual points (one pont for each of the samples of the env_type that have a point for\n",
    "            # this sampling frequency\n",
    "            y_list = env_type_df[col].dropna().tolist()\n",
    "            x_list = [col for i in range(len(y_list))]\n",
    "            axarr[axx_ind].scatter(x_list, y_list, marker='.' , color=colour_dict[env_type], s=1)\n",
    "            axarr[axx_ind].text(x_list[0], max(y_list) + 10, str(len(y_list)), fontsize=8, ha='center')\n",
    "\n",
    "            # only plot the line point if the number of samples remaining is > 1/3 of the total samples\n",
    "            if len(y_list) < num_samples/3:\n",
    "                break\n",
    "            mean_y.append(sum(y_list)/len(y_list))\n",
    "            mean_x.append(x_list[0])\n",
    "\n",
    "        # now draw the line\n",
    "        apples = 'asdf'\n",
    "        axarr[axx_ind].plot(mean_x, mean_y, color=colour_dict[env_type])\n",
    "        line_holder.append((mean_x, mean_y, colour_dict[env_type]))\n",
    "        # plt.show()\n",
    "        apples = 'asdf'\n",
    "\n",
    "        axarr[axx_ind].set_xlabel(env_type)\n",
    "        axx_ind += 1\n",
    "\n",
    "    for tup in line_holder:\n",
    "        axarr[5].plot(tup[0], tup[1], color=tup[2])\n",
    "        axarr[5].set_xlabel('all')\n",
    "\n",
    "    # ticks_list = [10 ** i for i in range(6)]\n",
    "    # plt.set_xticks = [10 ** i for i in range(2, 6)]\n",
    "\n",
    "    # NB getting the axes to behave well when logged was tricky but the below link gave a good solution\n",
    "\n",
    "    axarr[0].set_xscale('log')\n",
    "    # formatter = FuncFormatter(log_10_product)\n",
    "    # axarr[0].xaxis.set_major_formatter(formatter)\n",
    "    # axarr[0].set_xlim(1e-1, 1e5)\n",
    "    locmaj = matplotlib.ticker.LogLocator(base=10, numticks=12)\n",
    "    axarr[0].xaxis.set_major_locator(locmaj)\n",
    "    # axarr[0].set_xticks = [10 ** i for i in range(2, 6)]\n",
    "    # axarr[0].set_xticklabels = [10 ** i for i in range(2, 6)]\n",
    "    locmin = matplotlib.ticker.LogLocator(base=10.0, subs=(0.2, 0.4, 0.6, 0.8), numticks=12)\n",
    "    axarr[0].xaxis.set_minor_locator(locmin)\n",
    "    axarr[0].xaxis.set_minor_formatter(matplotlib.ticker.NullFormatter())\n",
    "\n",
    "    grid(True)\n",
    "    # plt.xlabel('sampling point log10')\n",
    "    # plt.ylabel('unique sequences')\n",
    "    fig.text(0.5, 0.02, 'sampling point log10', ha='center', va='center')\n",
    "    fig.text(0.06, 0.5, 'unique sequences', ha='center', va='center', rotation='vertical')\n",
    "    # plt.tight_layout()\n",
    "    plt.savefig('rarefaction_one_third_cutoff.svg')\n",
    "    plt.savefig('rarefaction_one_third_cutoff.png')\n",
    "    apples = 'asdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_elements_text_list(dict_list):\n",
    "    # if the dict_list is only 2 elements long then we are working out a 2 venn\n",
    "    # else we are working out a 3 venn\n",
    "\n",
    "    if len(dict_list) == 2:\n",
    "        dict_one_set = set(dict_list[0].keys())\n",
    "        dict_two_set = set(dict_list[1].keys())\n",
    "        dict_one_sum = sum(dict_list[0].values())\n",
    "        dict_two_sum = sum(dict_list[1].values())\n",
    "        union_list = \\\n",
    "            [\n",
    "                dict_one_set.difference(dict_two_set),\n",
    "                dict_one_set.intersection(dict_two_set),\n",
    "                dict_two_set.difference(dict_one_set)\n",
    "            ]\n",
    "        list_of_all_seqs = list(set(dict_list[0].keys()) | set(dict_list[1].keys()))\n",
    "        # return a list of labels in the order of 10 11 01\n",
    "        label_list_info = [[0 for j in range(len(dict_list))] for i in range(3)]\n",
    "        for seq_name in list_of_all_seqs:\n",
    "            if seq_name in union_list[0]:\n",
    "                # then it is only in 10\n",
    "                label_list_info[0][0] += dict_list[0][seq_name]\n",
    "            elif seq_name in union_list[1]:\n",
    "                # then it is in both of the dicts (11)\n",
    "                label_list_info[1][0] += dict_list[0][seq_name]\n",
    "                label_list_info[1][1] += dict_list[1][seq_name]\n",
    "            elif seq_name in union_list[2]:\n",
    "                # then the seq is only in 01\n",
    "                label_list_info[2][1] += dict_list[1][seq_name]\n",
    "\n",
    "        # At this point we have the label list populated\n",
    "        # now we can generate the label for eachof the segments\n",
    "        # order: 10, 11, 01\n",
    "        label_list_name = []\n",
    "        # 10\n",
    "        label_list_name.append((len(union_list[0]),'{:.2f}:{:.2f}'.format(label_list_info[0][0]/dict_one_sum, 0)))\n",
    "        # 11\n",
    "        label_list_name.append((len(union_list[1]), '{:.2f}:{:.2f}'.format(label_list_info[1][0]/dict_one_sum, label_list_info[1][1]/dict_two_sum)))\n",
    "        # 01\n",
    "        label_list_name.append((len(union_list[2]), '{:.2f}:{:.2f}'.format(0, label_list_info[2][1] / dict_two_sum)))\n",
    "        return label_list_name\n",
    "    else:\n",
    "        ## here we are dealing with a venn 3\n",
    "        # element_list = ['100', '110', '010', '011', '001', '101', '111']\n",
    "        dict_one_set = set(dict_list[0].keys())\n",
    "        dict_two_set = set(dict_list[1].keys())\n",
    "        dict_three_set = set(dict_list[2].keys())\n",
    "        dict_one_sum = sum(dict_list[0].values())\n",
    "        dict_two_sum = sum(dict_list[1].values())\n",
    "        dict_three_sum = sum(dict_list[2].values())\n",
    "\n",
    "        union_list = \\\n",
    "            [\n",
    "                #100\n",
    "                dict_one_set.difference(dict_two_set).difference(dict_three_set),\n",
    "                #110\n",
    "                dict_one_set.intersection(dict_two_set).difference(dict_three_set),\n",
    "                #010\n",
    "                dict_two_set.difference(dict_one_set).difference(dict_three_set),\n",
    "                #011\n",
    "                dict_two_set.intersection(dict_three_set).difference(dict_one_set),\n",
    "                #001\n",
    "                dict_three_set.difference(dict_one_set).difference(dict_two_set),\n",
    "                #101\n",
    "                dict_one_set.intersection(dict_three_set).difference(dict_two_set),\n",
    "                #111\n",
    "                dict_one_set.intersection(dict_three_set).intersection(dict_two_set)\n",
    "            ]\n",
    "        list_of_all_seqs = list(set(dict_list[0].keys()) | set(dict_list[1].keys()) | set(dict_list[2].keys()))\n",
    "        # return a list of labels in the order of '100', '110', '010', '011', '001', '101', '111'\n",
    "        label_list_info = [[0 for j in range(len(dict_list))] for i in range(7)]\n",
    "        for seq_name in list_of_all_seqs:\n",
    "            if seq_name in union_list[0]:\n",
    "                # then it is only in 100\n",
    "                label_list_info[0][0] += dict_list[0][seq_name]\n",
    "            elif seq_name in union_list[1]:\n",
    "                # then it is in both of the dicts (110)\n",
    "                label_list_info[1][0] += dict_list[0][seq_name]\n",
    "                label_list_info[1][1] += dict_list[1][seq_name]\n",
    "            elif seq_name in union_list[2]:\n",
    "                # then the seq is only in 010\n",
    "                label_list_info[2][1] += dict_list[1][seq_name]\n",
    "            elif seq_name in union_list[3]:\n",
    "                # then it is in both of the dicts (011)\n",
    "                label_list_info[3][1] += dict_list[1][seq_name]\n",
    "                label_list_info[3][2] += dict_list[2][seq_name]\n",
    "            elif seq_name in union_list[4]:\n",
    "                # then the seq is only in 001\n",
    "                label_list_info[4][2] += dict_list[2][seq_name]\n",
    "            elif seq_name in union_list[5]:\n",
    "                # then it is in both of the dicts (101)\n",
    "                label_list_info[5][0] += dict_list[0][seq_name]\n",
    "                label_list_info[5][2] += dict_list[2][seq_name]\n",
    "            elif seq_name in union_list[6]:\n",
    "                # then the seq is in all three dicts 111\n",
    "                label_list_info[6][0] += dict_list[0][seq_name]\n",
    "                label_list_info[6][1] += dict_list[1][seq_name]\n",
    "                label_list_info[6][2] += dict_list[2][seq_name]\n",
    "\n",
    "        # At this point we have the label list populated\n",
    "        # now we can generate the label for eachof the segments\n",
    "        # order: '100', '110', '010', '011', '001', '101', '111'\n",
    "        label_list_name = []\n",
    "        # 100\n",
    "        label_list_name.append((\n",
    "                len(union_list[0]),\n",
    "                '{:.2f}:{:.2f}:{:.2f}'.format(label_list_info[0][0] / dict_one_sum, 0, 0)\n",
    "            ))\n",
    "        # 110\n",
    "        label_list_name.append((\n",
    "            len(union_list[1]),\n",
    "            '{:.2f}:{:.2f}:{:.2f}'.format(\n",
    "                label_list_info[1][0] / dict_one_sum,\n",
    "                label_list_info[1][1] / dict_two_sum,\n",
    "                0)))\n",
    "        # 010\n",
    "        label_list_name.append((\n",
    "            len(union_list[2]),\n",
    "            '{:.2f}:{:.2f}:{:.2f}'.format(0, label_list_info[2][1] / dict_two_sum, 0)))\n",
    "        # 011\n",
    "        label_list_name.append((\n",
    "            len(union_list[1]),\n",
    "            '{:.2f}:{:.2f}:{:.2f}'.format(\n",
    "                0,\n",
    "                label_list_info[3][1] / dict_two_sum,\n",
    "                label_list_info[3][2] / dict_three_sum)\n",
    "        ))\n",
    "        # 001\n",
    "        label_list_name.append((\n",
    "            len(union_list[2]),\n",
    "            '{:.2f}:{:.2f}:{:.2f}'.format(0, 0, label_list_info[4][2] / dict_three_sum)))\n",
    "        # 101\n",
    "        label_list_name.append((\n",
    "            len(union_list[1]),\n",
    "            '{:.2f}:{:.2f}:{:.2f}'.format(\n",
    "                label_list_info[5][0] / dict_one_sum,\n",
    "                0,\n",
    "                label_list_info[5][2] / dict_three_sum)\n",
    "        ))\n",
    "        # 111\n",
    "        label_list_name.append((\n",
    "            len(union_list[2]),\n",
    "            '{:.2f}:{:.2f}:{:.2f}'.format(\n",
    "                label_list_info[6][0] / dict_one_sum,\n",
    "                label_list_info[6][1] / dict_two_sum,\n",
    "                label_list_info[6][2] / dict_three_sum)))\n",
    "        return label_list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_colour_list():\n",
    "    colour_list = [\"#FFFF00\", \"#1CE6FF\", \"#FF34FF\", \"#FF4A46\", \"#008941\", \"#006FA6\", \"#A30059\", \"#FFDBE5\",\n",
    "                  \"#7A4900\", \"#0000A6\", \"#63FFAC\", \"#B79762\", \"#004D43\", \"#8FB0FF\", \"#997D87\", \"#5A0007\", \"#809693\",\n",
    "                  \"#FEFFE6\", \"#1B4400\", \"#4FC601\", \"#3B5DFF\", \"#4A3B53\", \"#FF2F80\", \"#61615A\", \"#BA0900\", \"#6B7900\",\n",
    "                  \"#00C2A0\", \"#FFAA92\", \"#FF90C9\", \"#B903AA\", \"#D16100\", \"#DDEFFF\", \"#000035\", \"#7B4F4B\", \"#A1C299\",\n",
    "                  \"#300018\", \"#0AA6D8\", \"#013349\", \"#00846F\", \"#372101\", \"#FFB500\", \"#C2FFED\", \"#A079BF\", \"#CC0744\",\n",
    "                  \"#C0B9B2\", \"#C2FF99\", \"#001E09\", \"#00489C\", \"#6F0062\", \"#0CBD66\", \"#EEC3FF\", \"#456D75\", \"#B77B68\",\n",
    "                  \"#7A87A1\", \"#788D66\", \"#885578\", \"#FAD09F\", \"#FF8A9A\", \"#D157A0\", \"#BEC459\", \"#456648\", \"#0086ED\",\n",
    "                  \"#886F4C\", \"#34362D\", \"#B4A8BD\", \"#00A6AA\", \"#452C2C\", \"#636375\", \"#A3C8C9\", \"#FF913F\", \"#938A81\",\n",
    "                  \"#575329\", \"#00FECF\", \"#B05B6F\", \"#8CD0FF\", \"#3B9700\", \"#04F757\", \"#C8A1A1\", \"#1E6E00\", \"#7900D7\",\n",
    "                  \"#A77500\", \"#6367A9\", \"#A05837\", \"#6B002C\", \"#772600\", \"#D790FF\", \"#9B9700\", \"#549E79\", \"#FFF69F\",\n",
    "                  \"#201625\", \"#72418F\", \"#BC23FF\", \"#99ADC0\", \"#3A2465\", \"#922329\", \"#5B4534\", \"#FDE8DC\", \"#404E55\",\n",
    "                  \"#0089A3\", \"#CB7E98\", \"#A4E804\", \"#324E72\", \"#6A3A4C\", \"#83AB58\", \"#001C1E\", \"#D1F7CE\", \"#004B28\",\n",
    "                  \"#C8D0F6\", \"#A3A489\", \"#806C66\", \"#222800\", \"#BF5650\", \"#E83000\", \"#66796D\", \"#DA007C\", \"#FF1A59\",\n",
    "                  \"#8ADBB4\", \"#1E0200\", \"#5B4E51\", \"#C895C5\", \"#320033\", \"#FF6832\", \"#66E1D3\", \"#CFCDAC\", \"#D0AC94\",\n",
    "                  \"#7ED379\", \"#012C58\", \"#7A7BFF\", \"#D68E01\", \"#353339\", \"#78AFA1\", \"#FEB2C6\", \"#75797C\", \"#837393\",\n",
    "                  \"#943A4D\", \"#B5F4FF\", \"#D2DCD5\", \"#9556BD\", \"#6A714A\", \"#001325\", \"#02525F\", \"#0AA3F7\", \"#E98176\",\n",
    "                  \"#DBD5DD\", \"#5EBCD1\", \"#3D4F44\", \"#7E6405\", \"#02684E\", \"#962B75\", \"#8D8546\", \"#9695C5\", \"#E773CE\",\n",
    "                  \"#D86A78\", \"#3E89BE\", \"#CA834E\", \"#518A87\", \"#5B113C\", \"#55813B\", \"#E704C4\", \"#00005F\", \"#A97399\",\n",
    "                  \"#4B8160\", \"#59738A\", \"#FF5DA7\", \"#F7C9BF\", \"#643127\", \"#513A01\", \"#6B94AA\", \"#51A058\", \"#A45B02\",\n",
    "                  \"#1D1702\", \"#E20027\", \"#E7AB63\", \"#4C6001\", \"#9C6966\", \"#64547B\", \"#97979E\", \"#006A66\", \"#391406\",\n",
    "                  \"#F4D749\", \"#0045D2\", \"#006C31\", \"#DDB6D0\", \"#7C6571\", \"#9FB2A4\", \"#00D891\", \"#15A08A\", \"#BC65E9\",\n",
    "                  \"#FFFFFE\", \"#C6DC99\", \"#203B3C\", \"#671190\", \"#6B3A64\", \"#F5E1FF\", \"#FFA0F2\", \"#CCAA35\", \"#374527\",\n",
    "                  \"#8BB400\", \"#797868\", \"#C6005A\", \"#3B000A\", \"#C86240\", \"#29607C\", \"#402334\", \"#7D5A44\", \"#CCB87C\",\n",
    "                  \"#B88183\", \"#AA5199\", \"#B5D6C3\", \"#A38469\", \"#9F94F0\", \"#A74571\", \"#B894A6\", \"#71BB8C\", \"#00B433\",\n",
    "                  \"#789EC9\", \"#6D80BA\", \"#953F00\", \"#5EFF03\", \"#E4FFFC\", \"#1BE177\", \"#BCB1E5\", \"#76912F\", \"#003109\",\n",
    "                  \"#0060CD\", \"#D20096\", \"#895563\", \"#29201D\", \"#5B3213\", \"#A76F42\", \"#89412E\", \"#1A3A2A\", \"#494B5A\",\n",
    "                  \"#A88C85\", \"#F4ABAA\", \"#A3F3AB\", \"#00C6C8\", \"#EA8B66\", \"#958A9F\", \"#BDC9D2\", \"#9FA064\", \"#BE4700\",\n",
    "                  \"#658188\", \"#83A485\", \"#453C23\", \"#47675D\", \"#3A3F00\", \"#061203\", \"#DFFB71\", \"#868E7E\", \"#98D058\",\n",
    "                  \"#6C8F7D\", \"#D7BFC2\", \"#3C3E6E\", \"#D83D66\", \"#2F5D9B\", \"#6C5E46\", \"#D25B88\", \"#5B656C\", \"#00B57F\",\n",
    "                  \"#545C46\", \"#866097\", \"#365D25\", \"#252F99\", \"#00CCFF\", \"#674E60\", \"#FC009C\", \"#92896B\"]\n",
    "    return colour_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
